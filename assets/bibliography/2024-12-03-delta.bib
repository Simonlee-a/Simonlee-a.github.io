@inproceedings{vorontsov_orthogonality_2016,
 author = {Eugene Vorontsov and
Chiheb Trabelsi and
Samuel Kadoury and
Chris Pal},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/VorontsovTKP17.bib},
 booktitle = {Proceedings of the 34th International Conference on Machine Learning,
{ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017},
 editor = {Doina Precup and
Yee Whye Teh},
 pages = {3570--3578},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Wed, 03 Apr 2019 01:00:00 +0200},
 title = {On orthogonality and learning recurrent networks with long term dependencies},
 url = {http://proceedings.mlr.press/v70/vorontsov17a.html},
 volume = {70},
 year = {2017}
}

@article{Yang2022TensorPV,
 author = {Greg Yang and J. Edward Hu and Igor Babuschkin and Szymon Sidor and Xiaodong Liu and David Farhi and Nick Ryder and Jakub W. Pachocki and Weizhu Chen and Jianfeng Gao},
 journal = {ArXiv preprint},
 title = {Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer},
 url = {https://arxiv.org/abs/2203.03466},
 volume = {abs/2203.03466},
 year = {2022}
}
@article{Xu2024KVSA,
  title={KV Shifting Attention Enhances Language Modeling},
  author={Mingyu Xu and Wei Cheng and Bingning Wang and Weipeng Chen},
  journal={ArXiv},
  year={2024},
  volume={abs/2411.19574},
  url={https://api.semanticscholar.org/CorpusID:274422840}
}

@article{Gardner1988TheSO,
 author = {E. Gardner},
 journal = {Journal of Physics A},
 pages = {257-270},
 title = {The space of interactions in neural network models},
 volume = {21},
 year = {1988}
}

@article{Hu2024MiniCPMUT,
 author = {Shengding Hu and Yuge Tu and Xu Han and Chaoqun He and Ganqu Cui and Xiang Long and Zhi Zheng and Yewei Fang and Yuxiang Huang and Weilin Zhao and Xinrong Zhang and Zhen Leng Thai and Kaihuo Zhang and Chongyi Wang and Yuan Yao and Chenyang Zhao and Jie Zhou and Jie Cai and Zhongwu Zhai and Ning Ding and Chaochao Jia and Guoyang Zeng and Dahai Li and Zhiyuan Liu and Maosong Sun},
 journal = {ArXiv preprint},
 title = {MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies},
 url = {https://arxiv.org/abs/2404.06395},
 volume = {abs/2404.06395},
 year = {2024}
}

@article{Dubey2024TheL3,
 author = {Llama3 Team},
 journal = {ArXiv preprint},
 title = {The Llama 3 Herd of Models},
 url = {https://arxiv.org/abs/2407.21783},
 volume = {abs/2407.21783},
 year = {2024}
}

@article{Botev2024RecurrentGemmaMP,
 author = {Griffin, RLHF and Gemma Teams},
 journal = {ArXiv preprint},
 title = {RecurrentGemma: Moving Past Transformers for Efficient Open Language Models},
 url = {https://arxiv.org/abs/2404.07839},
 volume = {abs/2404.07839},
 year = {2024}
}

@article{Shen2024PowerSA,
 author = {Yikang Shen and Matt Stallone and Mayank Mishra and Gaoyuan Zhang and Shawn Tan and Aditya Prasad and Adriana Meza Soria and David D. Cox and Rameswar Panda},
 journal = {ArXiv preprint},
 title = {Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler},
 url = {https://arxiv.org/abs/2408.13359},
 volume = {abs/2408.13359},
 year = {2024}
}

@inproceedings{Peng2018RationalR,
 address = {Brussels, Belgium},
 author = {Peng, Hao  and
Schwartz, Roy  and
Thomson, Sam  and
Smith, Noah A.},
 booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/D18-1152},
 editor = {Riloff, Ellen  and
Chiang, David  and
Hockenmaier, Julia  and
Tsujii, Jun{'}ichi},
 pages = {1203--1214},
 publisher = {Association for Computational Linguistics},
 title = {Rational Recurrences},
 url = {https://aclanthology.org/D18-1152},
 year = {2018}
}

@misc{ali2024hidden,
 archiveprefix = {arXiv},
 author = {Ameen Ali and Itamar Zimerman and Lior Wolf},
 eprint = {2403.01590},
 primaryclass = {cs.LG},
 title = {The Hidden Attention of Mamba Models},
 year = {2024}
}

@article{Feng2024AttentionAA,
 author = {Leo Feng and Frederick Tung and Hossein Hajimirsadeghi and Mohamed Osama Ahmed and Yoshua Bengio and Greg Mori},
 journal = {ArXiv preprint},
 title = {Attention as an RNN},
 url = {https://arxiv.org/abs/2405.13956},
 volume = {abs/2405.13956},
 year = {2024}
}

@article{DBLP:journals/corr/abs-2405-16504,
 author = {Itamar Zimerman and
Ameen Ali and
Lior Wolf},
 journal = {ArXiv preprint},
 title = {A Unified Implicit Attention Formulation for Gated-Linear Recurrent
Sequence Models},
 url = {https://arxiv.org/abs/2405.16504},
 volume = {abs/2405.16504},
 year = {2024}
}

@inproceedings{irie2022dual,
 author = {Kazuki Irie and
R{\'{o}}bert Csord{\'{a}}s and
J{\"{u}}rgen Schmidhuber},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/IrieCS22.bib},
 booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
2022, Baltimore, Maryland, {USA}},
 editor = {Kamalika Chaudhuri and
Stefanie Jegelka and
Le Song and
Csaba Szepesv{\'{a}}ri and
Gang Niu and
Sivan Sabato},
 pages = {9639--9659},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Tue, 12 Jul 2022 01:00:00 +0200},
 title = {The Dual Form of Neural Networks Revisited: Connecting Test Time Predictions
to Training Patterns via Spotlights of Attention},
 url = {https://proceedings.mlr.press/v162/irie22a.html},
 volume = {162},
 year = {2022}
}

@article{liu-2024-longhorn,
 author = {Bo Liu and Rui Wang and Lemeng Wu and Yihao Feng and Peter Stone and Qian Liu},
 journal = {ArXiv preprint},
 title = {Longhorn: State Space Models are Amortized Online Learners},
 url = {https://arxiv.org/abs/2407.14207},
 volume = {abs/2407.14207},
 year = {2024}
}

@misc{arora-2024-jrt,
 author = {Simran Arora and Aman Timalsina and Aaryan Singhal and Benjamin Spector and Sabri Eyuboglu and Xinyi Zhao and Ashish Rao and Atri Rudra and Christopher Ré},
 journal = {ArXiv preprint},
 title = {Just read twice: closing the recall gap for recurrent language models},
 url = {https://arxiv.org/abs/2407.05483},
 volume = {abs/2407.05483},
 year = {2024}
}

@article{sun-2024-learning,
 author = {Yu Sun and Xinhao Li and Karan Dalal and Jiarui Xu and Arjun Vikram and Genghan Zhang and Yann Dubois and Xinlei Chen and Xiaolong Wang and Oluwasanmi Koyejo and Tatsunori Hashimoto and Carlos Guestrin},
 journal = {ArXiv preprint},
 title = {Learning to (Learn at Test Time): RNNs with Expressive Hidden States},
 url = {https://arxiv.org/abs/2407.04620},
 volume = {abs/2407.04620},
 year = {2024}
}

@inproceedings{zhang2024gated,
 author = {Yu Zhang and Songlin Yang and Ruijie Zhu and Yue Zhang and Leyang Cui and Yiqiao Wang and Bolun Wang and Freda Shi and Bailin Wang and Wei Bi and Peng Zhou and Guohong Fu},
 booktitle = {The Thirty-eighth Annual Conference on Neural Information Processing Systems},
 title = {Gated Slot Attention for Efficient Linear-Time Sequence Modeling},
 year = {2024}
}

@article{Joffrain2006AccumulatingHT,
 author = {Thierry Joffrain and Tze Meng Low and Enrique S. Quintana-Ort{\'i} and Robert A. van de Geijn and Field G. Van Zee},
 journal = {ACM Trans. Math. Softw.},
 pages = {169-179},
 title = {Accumulating Householder transformations, revisited},
 url = {https://api.semanticscholar.org/CorpusID:15723171},
 volume = {32},
 year = {2006}
}

@article{TomsDominguez2018FastBO,
 author = {Andr{\'e}s E. Tom{\'a}s Dominguez and Enrique S. Quintana Orti},
 journal = {2018 26th Euromicro International Conference on Parallel, Distributed and Network-based Processing (PDP)},
 pages = {385-393},
 title = {Fast Blocking of Householder Reflectors on Graphics Processors},
 url = {https://api.semanticscholar.org/CorpusID:46960439},
 year = {2018}
}

@article{ren2024samba,
 author = {Ren, Liliang and Liu, Yang and Lu, Yadong and Shen, Yelong and Liang, Chen and Chen, Weizhu},
 journal = {ArXiv preprint},
 title = {Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling},
 url = {https://arxiv.org/abs/2406.07522},
 volume = {abs/2406.07522},
 year = {2024}
}

@inproceedings{helfrich_orthogonal_2018,
 author = {Kyle Helfrich and
Devin Willmott and
Qiang Ye},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/HelfrichWY18.bib},
 booktitle = {Proceedings of the 35th International Conference on Machine Learning,
{ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
2018},
 editor = {Jennifer G. Dy and
Andreas Krause},
 pages = {1974--1983},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Mon, 23 Mar 2020 00:00:00 +0100},
 title = {Orthogonal Recurrent Neural Networks with Scaled Cayley Transform},
 url = {http://proceedings.mlr.press/v80/helfrich18a.html},
 volume = {80},
 year = {2018}
}

@article{jing_gated_2019,
 abstract = {We present a novel recurrent neural network (RNN)–based model that combines the remembering ability of unitary evolution RNNs with the ability of gated RNNs to effectively forget redundant or irrelevant information in its memory. We achieve this by extending restricted orthogonal evolution RNNs with a gating mechanism similar to gated recurrent unit RNNs with a reset gate and an update gate. Our model is able to outperform long short-term memory, gated recurrent units, and vanilla unitary or orthogonal RNNs on several long-term-dependency benchmark tasks. We empirically show that both orthogonal and unitary RNNs lack the ability to forget. This ability plays an important role in RNNs. We provide competitive results along with an analysis of our model on many natural sequential tasks, including question answering, speech spectrum prediction, character-level language modeling, and synthetic tasks that involve long-term dependencies such as algorithmic, denoising, and copying tasks.},
 author = {Jing, Li and Gulcehre, Caglar and Peurifoy, John and Shen, Yichen and Tegmark, Max and Soljacic, Marin and Bengio, Yoshua},
 doi = {10.1162/neco_a_01174},
 issn = {0899-7667, 1530-888X},
 journal = {Neural Computation},
 language = {en},
 number = {4},
 pages = {765--783},
 shorttitle = {Gated {Orthogonal} {Recurrent} {Units}},
 title = {Gated {Orthogonal} {Recurrent} {Units}: {On} {Learning} to {Forget}},
 url = {https://direct.mit.edu/neco/article/31/4/765-783/8458},
 urldate = {2024-06-06},
 volume = {31},
 year = {2019}
}

@inproceedings{berg_sylvester_2019,
 author = {Rianne van den Berg and
Leonard Hasenclever and
Jakub M. Tomczak and
Max Welling},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/uai/BergHTW18.bib},
 booktitle = {Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial
Intelligence, {UAI} 2018, Monterey, California, USA, August 6-10,
2018},
 editor = {Amir Globerson and
Ricardo Silva},
 pages = {393--402},
 publisher = {{AUAI} Press},
 timestamp = {Thu, 12 Mar 2020 00:00:00 +0100},
 title = {Sylvester Normalizing Flows for Variational Inference},
 url = {http://auai.org/uai2018/proceedings/papers/156.pdf},
 year = {2018}
}

@misc{qin_linearized_2023,
 author = {Qin, Zhen and Sun, Weixuan and Lu, Kaiyue and Deng, Hui and Li, Dongxu and Han, Xiaodong and Dai, Yuchao and Kong, Lingpeng and Zhong, Yiran},
 journal = {ArXiv preprint},
 title = {Linearized {Relative} {Positional} {Encoding}},
 url = {https://arxiv.org/abs/2307.09270},
 volume = {abs/2307.09270},
 year = {2023}
}

@inproceedings{zhang_stabilizing_2018,
 author = {Jiong Zhang and
Qi Lei and
Inderjit S. Dhillon},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/ZhangLD18.bib},
 booktitle = {Proceedings of the 35th International Conference on Machine Learning,
{ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
2018},
 editor = {Jennifer G. Dy and
Andreas Krause},
 pages = {5801--5809},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Wed, 03 Apr 2019 01:00:00 +0200},
 title = {Stabilizing Gradients for Deep Neural Networks via Efficient {SVD}
Parameterization},
 url = {http://proceedings.mlr.press/v80/zhang18g.html},
 volume = {80},
 year = {2018}
}

@misc{tomczak_improving_2017,
 author = {Tomczak, Jakub M. and Welling, Max},
 journal = {ArXiv preprint},
 title = {Improving {Variational} {Auto}-{Encoders} using {Householder} {Flow}},
 url = {https://arxiv.org/abs/1611.09630},
 volume = {abs/1611.09630},
 year = {2016}
}
@article{olsson2022context,
   title={In-context Learning and Induction Heads},
   author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}
}

@misc{aksenov_linear_2024,
 author = {Aksenov, Yaroslav and Balagansky, Nikita and Vaina, Sofia Maria Lo Cicero and Shaposhnikov, Boris and Gorbatovski, Alexey and Gavrilov, Daniil},
 journal = {ArXiv preprint},
 title = {Linear {Transformers} with {Learnable} {Kernel} {Functions} are {Better} {In}-{Context} {Models}},
 url = {https://arxiv.org/abs/2402.10644},
 volume = {abs/2402.10644},
 year = {2024}
}

@inproceedings{ramsauer_hopfield_2021,
 author = {Hubert Ramsauer and
Bernhard Sch{\"{a}}fl and
Johannes Lehner and
Philipp Seidl and
Michael Widrich and
Lukas Gruber and
Markus Holzleitner and
Thomas Adler and
David P. Kreil and
Michael K. Kopp and
G{\"{u}}nter Klambauer and
Johannes Brandstetter and
Sepp Hochreiter},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/RamsauerSLSWGHA21.bib},
 booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
Virtual Event, Austria, May 3-7, 2021},
 publisher = {OpenReview.net},
 timestamp = {Wed, 23 Jun 2021 01:00:00 +0200},
 title = {Hopfield Networks is All You Need},
 url = {https://openreview.net/forum?id=tL89RnzIiCd},
 year = {2021}
}

@inproceedings{krotov_large_2021,
 author = {Dmitry Krotov and
John J. Hopfield},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/KrotovH21.bib},
 booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
Virtual Event, Austria, May 3-7, 2021},
 publisher = {OpenReview.net},
 timestamp = {Wed, 23 Jun 2021 01:00:00 +0200},
 title = {Large Associative Memory Problem in Neurobiology and Machine Learning},
 url = {https://openreview.net/forum?id=X4y\_10OX-hX},
 year = {2021}
}

@article{hopfield_neural_1982,
 abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
 author = {Hopfield, J J},
 doi = {10.1073/pnas.79.8.2554},
 journal = {Proceedings of the National Academy of Sciences},
 note = {Publisher: Proceedings of the National Academy of Sciences},
 number = {8},
 pages = {2554--2558},
 title = {Neural networks and physical systems with emergent collective computational abilities.},
 url = {https://www.pnas.org/doi/10.1073/pnas.79.8.2554},
 urldate = {2024-06-06},
 volume = {79},
 year = {1982}
}

@inproceedings{krotov_large_2021-1,
 author = {Dmitry Krotov and
John J. Hopfield},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/KrotovH21.bib},
 booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
Virtual Event, Austria, May 3-7, 2021},
 publisher = {OpenReview.net},
 timestamp = {Wed, 23 Jun 2021 01:00:00 +0200},
 title = {Large Associative Memory Problem in Neurobiology and Machine Learning},
 url = {https://openreview.net/forum?id=X4y\_10OX-hX},
 year = {2021}
}

@article{demircigil_model_2017,
 author = {Demircigil, Mete and Heusel, Judith and Löwe, Matthias and Upgang, Sven and Vermet, Franck},
 journal = {ArXiv preprint},
 title = {On a model of associative memory with huge storage capacity},
 url = {https://arxiv.org/abs/1702.01929},
 volume = {abs/1702.01929},
 year = {2017}
}

@misc{sun_linear_2024,
 author = {Sun, Weigao and Qin, Zhen and Li, Dong and Shen, Xuyang and Qiao, Yu and Zhong, Yiran},
 journal = {ArXiv preprint},
 title = {Linear {Attention} {Sequence} {Parallelism}},
 url = {https://arxiv.org/abs/2404.02882},
 volume = {abs/2404.02882},
 year = {2024}
}

@misc{noauthor_capacity_nodate,
 title = {The capacity of the {Hopfield} associative memory {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
 url = {https://ieeexplore.ieee.org/document/1057328},
 urldate = {2024-05-21}
}

@misc{nahshan_linear_2024,
 author = {Nahshan, Yury and Kampeas, Joseph and Haleva, Emir},
 journal = {ArXiv preprint},
 title = {Linear {Log}-{Normal} {Attention} with {Unbiased} {Concentration}},
 url = {https://arxiv.org/abs/2311.13541},
 volume = {abs/2311.13541},
 year = {2023}
}

@misc{millidge_linear_nodate,
 abstract = {In this short note, we present an equivalence and interpretation of the recurrent form of linear attention as implementing a continually updated hopfield network. Specifically, as the recurrent transformer is performing generation for each token, it simply adds a continuous ‘memory’ via Hebbian plasticity to a classical continuous hopfield network...},
 author = {Millidge, Beren},
 language = {en},
 title = {Linear {Attention} as {Iterated} {Hopfield} {Networks}},
 url = {http://www.beren.io/2024-03-03-Linear-Attention-as-Iterated-Hopfield-Networks/},
 urldate = {2024-05-21}
}

@inproceedings{mhammedi_efficient_2017,
 author = {Zakaria Mhammedi and
Andrew D. Hellicar and
Ashfaqur Rahman and
James Bailey},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/MhammediHRB17.bib},
 booktitle = {Proceedings of the 34th International Conference on Machine Learning,
{ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017},
 editor = {Doina Precup and
Yee Whye Teh},
 pages = {2401--2409},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Tue, 16 Apr 2019 01:00:00 +0200},
 title = {Efficient Orthogonal Parametrisation of Recurrent Neural Networks
Using Householder Reflections},
 url = {http://proceedings.mlr.press/v70/mhammedi17a.html},
 volume = {70},
 year = {2017}
}

@inproceedings{paperno_lambada_2016,
 address = {Berlin, Germany},
 author = {Paperno, Denis  and
Kruszewski, Germ{\'a}n  and
Lazaridou, Angeliki  and
Pham, Ngoc Quan  and
Bernardi, Raffaella  and
Pezzelle, Sandro  and
Baroni, Marco  and
Boleda, Gemma  and
Fern{\'a}ndez, Raquel},
 booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/P16-1144},
 editor = {Erk, Katrin  and
Smith, Noah A.},
 pages = {1525--1534},
 publisher = {Association for Computational Linguistics},
 title = {The {LAMBADA} dataset: Word prediction requiring a broad discourse context},
 url = {https://aclanthology.org/P16-1144},
 year = {2016}
}

@inproceedings{lei_when_2021,
 address = {Online and Punta Cana, Dominican Republic},
 author = {Lei, Tao},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2021.emnlp-main.602},
 editor = {Moens, Marie-Francine  and
Huang, Xuanjing  and
Specia, Lucia  and
Yih, Scott Wen-tau},
 pages = {7633--7648},
 publisher = {Association for Computational Linguistics},
 title = {When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute},
 url = {https://aclanthology.org/2021.emnlp-main.602},
 year = {2021}
}

@inproceedings{huang_encoding_2022,
 author = {Feiqing Huang and
Kexin Lu and
Yuxi Cai and
Zhen Qin and
Yanwen Fang and
Guangjian Tian and
Guodong Li},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/HuangLCQFTL23.bib},
 booktitle = {The Eleventh International Conference on Learning Representations,
{ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
 publisher = {OpenReview.net},
 timestamp = {Fri, 30 Jun 2023 01:00:00 +0200},
 title = {Encoding Recurrence into Transformers},
 url = {https://openreview.net/pdf?id=7YfHla7IxBJ},
 year = {2023}
}

@misc{graves_neural_2014,
 abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efﬁciently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
 author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
 keywords = {Computer Science - Neural and Evolutionary Computing},
 language = {en},
 note = {arXiv:1410.5401 [cs]},
 publisher = {arXiv},
 title = {Neural {Turing} {Machines}},
 url = {http://arxiv.org/abs/1410.5401},
 urldate = {2024-05-20},
 year = {2014}
}
 

@inproceedings{gers_learning_1999,
 abstract = {Long short-term memory (LSTM) can solve many tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams without explicitly marked sequence ends. Without resets, the internal state values may grow indefinitely and eventually cause the network to break down. Our remedy is an adaptive "forget gate" that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review an illustrative benchmark problem on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve a continual version of that problem. LSTM with forget gates, however, easily solves it in an elegant way.},
 author = {Gers, F.A. and Schmidhuber, J. and Cummins, F.},
 booktitle = {1999 {Ninth} {International} {Conference} on {Artificial} {Neural} {Networks} {ICANN} 99. ({Conf}. {Publ}. {No}. 470)},
 doi = {10.1049/cp:19991218},
 note = {ISSN: 0537-9989},
 pages = {850--855 vol.2},
 shorttitle = {Learning to forget},
 title = {Learning to forget: continual prediction with {LSTM}},
 url = {https://ieeexplore.ieee.org/document/818041},
 urldate = {2024-05-20},
 volume = {2},
 year = {1999}
}

@misc{elfwing_sigmoid-weighted_2017,
 author = {Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
 journal = {ArXiv preprint},
 title = {Sigmoid-{Weighted} {Linear} {Units} for {Neural} {Network} {Function} {Approximation} in {Reinforcement} {Learning}},
 url = {https://arxiv.org/abs/1702.03118},
 volume = {abs/1702.03118},
 year = {2017}
}

@inproceedings{clevert_fast_2016,
 author = {Djork{-}Arn{\'{e}} Clevert and
Thomas Unterthiner and
Sepp Hochreiter},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/ClevertUH15.bib},
 booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
 editor = {Yoshua Bengio and
Yann LeCun},
 timestamp = {Sat, 23 Jan 2021 00:00:00 +0100},
 title = {Fast and Accurate Deep Network Learning by Exponential Linear Units
(ELUs)},
 url = {http://arxiv.org/abs/1511.07289},
 year = {2016}
}

@inproceedings{blelloch_prefix_2004,
 abstract = {Abstract: "Experienced algorithm designers rely heavily on a set of building blocks and on the tools needed to put the blocks together into an algorithm. The understanding of these basic blocks and tools is therefore critical to the understanding of algorithms. Many of the blocks and tools needed for parallel algorithms extend from sequential algorithms, such as dynamic-programming and divide-and-conquer, but others are new. This paper introduces one of the simplest and most useful building blocks for parallel algorithms: the all-prefix-sums operation. The paper defines the operation, shows how to implement it on a P-RAM and illustrates many applications of the operation.In addition to being a useful building block, the all-prefix-sums operation is a good example of a computation that seems inherently sequential, but for which there is an efficient parallel algorithm."},
 author = {Blelloch, Guy E.},
 copyright = {In Copyright},
 doi = {10.1184/R1/6608579.V1},
 keywords = {89999 Information and Computing Sciences not elsewhere classified, FOS: Computer and information sciences},
 note = {Artwork Size: 1294199 Bytes},
 pages = {1294199 Bytes},
 publisher = {Carnegie Mellon University},
 title = {Prefix sums and their applications},
 url = {https://kilthub.cmu.edu/articles/Prefix_sums_and_their_applications/6608579/1},
 urldate = {2024-05-20},
 year = {2004}
}

@inproceedings{lewis_retrieval-augmented_2021,
 author = {Patrick S. H. Lewis and
Ethan Perez and
Aleksandra Piktus and
Fabio Petroni and
Vladimir Karpukhin and
Naman Goyal and
Heinrich K{\"{u}}ttler and
Mike Lewis and
Wen{-}tau Yih and
Tim Rockt{\"{a}}schel and
Sebastian Riedel and
Douwe Kiela},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/LewisPPPKGKLYR020.bib},
 booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
 editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
 timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
 title = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
 url = {https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html},
 year = {2020}
}

@misc{botev_recurrentgemma_2024,
 author = {Botev, Aleksandar and De, Soham and Smith, Samuel L. and Fernando, Anushan and Muraru, George-Cristian and Haroun, Ruba and Berrada, Leonard and Pascanu, Razvan and Sessa, Pier Giuseppe and Dadashi, Robert and Hussenot, Léonard and Ferret, Johan and Girgin, Sertan and Bachem, Olivier and Andreev, Alek and Kenealy, Kathleen and Mesnard, Thomas and Hardin, Cassidy and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivière, Morgane and Kale, Mihir Sanjay and Love, Juliette and Tafti, Pouya and Joulin, Armand and Fiedel, Noah and Senter, Evan and Chen, Yutian and Srinivasan, Srivatsan and Desjardins, Guillaume and Budden, David and Doucet, Arnaud and Vikram, Sharad and Paszke, Adam and Gale, Trevor and Borgeaud, Sebastian and Chen, Charlie and Brock, Andy and Paterson, Antonia and Brennan, Jenny and Risdal, Meg and Gundluru, Raj and Devanathan, Nesh and Mooney, Paul and Chauhan, Nilay and Culliton, Phil and Martins, Luiz GUStavo and Bandy, Elisa and Huntsperger, David and Cameron, Glenn and Zucker, Arthur and Warkentin, Tris and Peran, Ludovic and Giang, Minh and Ghahramani, Zoubin and Farabet, Clément and Kavukcuoglu, Koray and Hassabis, Demis and Hadsell, Raia and Teh, Yee Whye and de Frietas, Nando},
 journal = {ArXiv preprint},
 title = {{RecurrentGemma}: {Moving} {Past} {Transformers} for {Efficient} {Open} {Language} {Models}},
 url = {https://arxiv.org/abs/2404.07839},
 volume = {abs/2404.07839},
 year = {2024}
}

@inproceedings{irie_practical_2023,
 address = {Singapore},
 author = {Irie, Kazuki  and
Csord{\'a}s, R{\'o}bert  and
Schmidhuber, J{\"u}rgen},
 booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2023.emnlp-main.588},
 editor = {Bouamor, Houda  and
Pino, Juan  and
Bali, Kalika},
 pages = {9455--9465},
 publisher = {Association for Computational Linguistics},
 title = {Practical Computational Power of Linear Transformers and Their Recurrent and Self-Referential Extensions},
 url = {https://aclanthology.org/2023.emnlp-main.588},
 year = {2023}
}

@inproceedings{fan_advancing_2024,
 address = {Mexico City, Mexico},
 author = {Fan, Ting-Han  and
Chi, Ta-Chung  and
Rudnicky, Alexander},
 booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)},
 editor = {Duh, Kevin  and
Gomez, Helena  and
Bethard, Steven},
 pages = {45--53},
 publisher = {Association for Computational Linguistics},
 title = {Advancing Regular Language Reasoning in Linear Recurrent Neural Networks},
 url = {https://aclanthology.org/2024.naacl-short.4},
 year = {2024}
}

@misc{merrill_illusion_2024,
 author = {Merrill, William and Petty, Jackson and Sabharwal, Ashish},
 journal = {ArXiv preprint},
 title = {The {Illusion} of {State} in {State}-{Space} {Models}},
 url = {https://arxiv.org/abs/2404.08819},
 volume = {abs/2404.08819},
 year = {2024}
}


@inproceedings{rajpurkar_know_2018,
 address = {Melbourne, Australia},
 author = {Rajpurkar, Pranav  and
Jia, Robin  and
Liang, Percy},
 booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 doi = {10.18653/v1/P18-2124},
 editor = {Gurevych, Iryna  and
Miyao, Yusuke},
 pages = {784--789},
 publisher = {Association for Computational Linguistics},
 title = {Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}},
 url = {https://aclanthology.org/P18-2124},
 year = {2018}
}

@inproceedings{lockard_openceres_2019,
 address = {Minneapolis, Minnesota},
 author = {Lockard, Colin  and
Shiralkar, Prashant  and
Dong, Xin Luna},
 booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
 doi = {10.18653/v1/N19-1309},
 editor = {Burstein, Jill  and
Doran, Christy  and
Solorio, Thamar},
 pages = {3047--3056},
 publisher = {Association for Computational Linguistics},
 title = {{O}pen{C}eres: {W}hen Open Information Extraction Meets the Semi-Structured Web},
 url = {https://aclanthology.org/N19-1309},
 year = {2019}
}

@article{wu_how_2021,
 abstract = {A comprehensive overview of medical AI devices approved by the US Food and Drug Administration sheds new light on limitations of the evaluation process that can mask vulnerabilities of devices when they are deployed on patients.},
 author = {Wu, Eric and Wu, Kevin and Daneshjou, Roxana and Ouyang, David and Ho, Daniel E. and Zou, James},
 doi = {10.1038/s41591-021-01312-x},
 issn = {1546-170X},
 journal = {Nature Medicine},
 number = {4},
 pages = {582--584},
 title = {How medical {AI} devices are evaluated: limitations and recommendations from an analysis of {FDA} approvals},
 url = {https://doi.org/10.1038/s41591-021-01312-x},
 volume = {27},
 year = {2021}
}

@misc{arora_language_2023,
 author = {Arora, Simran and Yang, Brandon and Eyuboglu, Sabri and Narayan, Avanika and Hojel, Andrew and Trummer, Immanuel and Ré, Christopher},
 journal = {ArXiv preprint},
 title = {Language {Models} {Enable} {Simple} {Systems} for {Generating} {Structured} {Views} of {Heterogeneous} {Data} {Lakes}},
 url = {https://arxiv.org/abs/2304.09433},
 volume = {abs/2304.09433},
 year = {2023}
}

@misc{de_griffin_2024,
 author = {De, Soham and Smith, Samuel L. and Fernando, Anushan and Botev, Aleksandar and Cristian-Muraru, George and Gu, Albert and Haroun, Ruba and Berrada, Leonard and Chen, Yutian and Srinivasan, Srivatsan and Desjardins, Guillaume and Doucet, Arnaud and Budden, David and Teh, Yee Whye and Pascanu, Razvan and De Freitas, Nando and Gulcehre, Caglar},
 journal = {ArXiv preprint},
 title = {Griffin: {Mixing} {Gated} {Linear} {Recurrences} with {Local} {Attention} for {Efficient} {Language} {Models}},
 url = {https://arxiv.org/abs/2402.19427},
 volume = {abs/2402.19427},
 year = {2024}
}

@article{mercat_linearizing_2024,
 author = {Mercat, Jean and Vasiljevic, Igor and Keh, Sedrick and Arora, Kushal and Dave, Achal and Gaidon, Adrien and Kollar, Thomas},
 journal = {ArXiv preprint},
 title = {Linearizing Large Language Models},
 url = {https://arxiv.org/abs/2405.06640},
 volume = {abs/2405.06640},
 year = {2024}
}

@misc{noauthor_deepspeed_nodate,
 abstract = {随着Gemini 1M context length和Sora出世，如何训练超长上下文的大模型引起了大家广泛关注。 文本对比两种目前炙手可热长文本训练方法DeepSpeed Ulysess和Ring-Attention。23年末，二者几乎同时出现，但是设计方法…},
 journal = {知乎专栏},
 language = {zh},
 title = {大模型训练之序列并行双雄：{DeepSpeed} {Ulysses} \& {Ring}-{Attention}},
 url = {https://zhuanlan.zhihu.com/p/689067888},
 urldate = {2024-05-12}
}

@misc{yang_fla_2024,
 abstract = {Efficient implementations of state-of-the-art linear attention models in Pytorch and Triton},
 author = {Yang, Songlin and Zhang, Yu},
 copyright = {MIT},
 note = {original-date: 2023-12-20T06:50:18Z},
 shorttitle = {{FLA}},
 title = {{FLA}: {A} {Triton}-{Based} {Library} for {Hardware}-{Efficient} {Implementations} of {Linear} {Attention} {Mechanism}},
 url = {https://github.com/sustcsonglin/flash-linear-attention},
 urldate = {2024-05-11},
 year = {2024}
}

@misc{noauthor_scholar_nodate,
 abstract = {Scholar Inbox is a personalized paper recommender enabling scholars to efficiently browse through the large amount of new research papers in AI that appear daily},
 language = {en},
 title = {Scholar {Inbox}},
 url = {https://www.scholar-inbox.com/trending},
 urldate = {2024-05-09}
}

@misc{beck_xlstm_2024,
 author = {Beck, Maximilian and Pöppel, Korbinian and Spanring, Markus and Auer, Andreas and Prudnikova, Oleksandra and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
 journal = {ArXiv preprint},
 title = {{xLSTM}: {Extended} {Long} {Short}-{Term} {Memory}},
 url = {https://arxiv.org/abs/2405.04517},
 volume = {abs/2405.04517},
 year = {2024}
}
@misc{ye2024differentialtransformer,
      title={Differential Transformer}, 
      author={Tianzhu Ye and Li Dong and Yuqing Xia and Yutao Sun and Yi Zhu and Gao Huang and Furu Wei},
      year={2024},
      eprint={2410.05258},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.05258}, 
}
@article{Qin2024VariousLC,
  title={Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention},
  author={Zhen Qin and Weigao Sun and Dong Li and Xuyang Shen and Weixuan Sun and Yiran Zhong},
  journal={ArXiv},
  year={2024},
  volume={abs/2405.17381},
  url={https://api.semanticscholar.org/CorpusID:270063820}
}
@inproceedings{
chou2024metala,
title={Meta{LA}: Unified Optimal Linear Approximation to Softmax Attention Map},
author={Yuhong Chou and Man Yao and Kexin Wang and Yuqi Pan and Rui-Jie Zhu and Jibin Wu and Yiran Zhong and Yu Qiao and Bo XU and Guoqi Li},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=Y8YVCOMEpz}
}
@misc{noauthor__nodate,
 title = {邮件 - {Songlin} {Yang} - {Outlook}},
 url = {https://outlook.office.com/mail/id/AAQkADkyNjU5OWQwLTllMmUtNGViOC1iYTQzLTY3MGFjNmI2OGQ0MgAQACzo4B3ay1JLvHBUIWVh2kw%3D},
 urldate = {2024-05-06}
}

@misc{poli_mechanistic_2024,
 author = {Poli, Michael and Thomas, Armin W. and Nguyen, Eric and Ponnusamy, Pragaash and Deiseroth, Björn and Kersting, Kristian and Suzuki, Taiji and Hie, Brian and Ermon, Stefano and Ré, Christopher and Zhang, Ce and Massaroli, Stefano},
 journal = {ArXiv preprint},
 title = {Mechanistic {Design} and {Scaling} of {Hybrid} {Architectures}},
 url = {https://arxiv.org/abs/2403.17844},
 volume = {abs/2403.17844},
 year = {2024}
}

@article{akyurek2024context,
 author = {Aky{\"u}rek, Ekin and Wang, Bailin and Kim, Yoon and Andreas, Jacob},
 journal = {ArXiv preprint},
 title = {In-context language learning: Arhitectures and algorithms},
 url = {https://arxiv.org/abs/2401.12973},
 volume = {abs/2401.12973},
 year = {2024}
}

@article{munkhdalai_metalearning_2018,
 abstract = {We unify recent neural approaches to one-shot learning with older ideas of associative memory in a model for metalearning. Our model learns jointly to represent data and to bind class labels to representations in a single shot. It builds representations via slow weights, learned across tasks through SGD, while fast weights constructed by a Hebbian learning rule implement one-shot binding for each new task. On the Omniglot, Mini-ImageNet, and Penn Treebank one-shot learning benchmarks, our model achieves state-of-the-art results.},
 author = {Munkhdalai, Tsendsuren and Trischler, Adam},
 journal = {ArXiv},
 title = {Metalearning with {Hebbian} {Fast} {Weights}},
 url = {https://www.semanticscholar.org/paper/b48f327d8f2515216ac19314a58292c43bb53422},
 urldate = {2024-05-05},
 year = {2018}
}

@inproceedings{munkhdalai_metalearned_2019,
 author = {Tsendsuren Munkhdalai and
Alessandro Sordoni and
Tong Wang and
Adam Trischler},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/MunkhdalaiSWT19.bib},
 booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019, NeurIPS 2019, December
8-14, 2019, Vancouver, BC, Canada},
 editor = {Hanna M. Wallach and
Hugo Larochelle and
Alina Beygelzimer and
Florence d'Alch{\'{e}}{-}Buc and
Emily B. Fox and
Roman Garnett},
 pages = {13310--13321},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Metalearned Neural Memory},
 url = {https://proceedings.neurips.cc/paper/2019/hash/182bd81ea25270b7d1c2fe8353d17fe6-Abstract.html},
 year = {2019}
}

@inproceedings{schlag_learning_2021,
 author = {Imanol Schlag and
Tsendsuren Munkhdalai and
J{\"{u}}rgen Schmidhuber},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/SchlagMS21.bib},
 booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
Virtual Event, Austria, May 3-7, 2021},
 publisher = {OpenReview.net},
 timestamp = {Wed, 23 Jun 2021 01:00:00 +0200},
 title = {Learning Associative Inference Using Fast Weight Memory},
 url = {https://openreview.net/forum?id=TuK6agbdt27},
 year = {2021}
}

@inproceedings{mathiasen_faster_nodate,
 author = {Mathiasen, Alexander and Hvilsh{\o}j, Frederik and J{\o}rgensen, Jakob R{\o}dsgaard and Nasery, Anshul and Mottin, Davide},
 booktitle = {ICML, Workshop Proceedings},
 title = {Faster Orthogonal Parameterization with Householder Matrices},
 year = {2020}
}

@misc{peng_eagle_2024,
 author = {Peng, Bo and Goldstein, Daniel and Anthony, Quentin and Albalak, Alon and Alcaide, Eric and Biderman, Stella and Cheah, Eugene and Du, Xingjian and Ferdinan, Teddy and Hou, Haowen and Kazienko, Przemysław and GV, Kranthi Kiran and Kocoń, Jan and Koptyra, Bartłomiej and Krishna, Satyapriya and McClelland Jr., Ronald and Muennighoff, Niklas and Obeid, Fares and Saito, Atsushi and Song, Guangyu and Tu, Haoqin and Woźniak, Stanisław and Zhang, Ruichong and Zhao, Bingchen and Zhao, Qihang and Zhou, Peng and Zhu, Jian and Zhu, Rui-Jie},
 journal = {ArXiv preprint},
 title = {Eagle and {Finch}: {RWKV} with {Matrix}-{Valued} {States} and {Dynamic} {Recurrence}},
 url = {https://arxiv.org/abs/2404.05892},
 volume = {abs/2404.05892},
 year = {2024}
}

@inproceedings{press_train_2021,
 author = {Ofir Press and
Noah A. Smith and
Mike Lewis},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/PressSL22.bib},
 booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
 publisher = {OpenReview.net},
 timestamp = {Tue, 27 Dec 2022 00:00:00 +0100},
 title = {Train Short, Test Long: Attention with Linear Biases Enables Input
Length Extrapolation},
 url = {https://openreview.net/forum?id=R8sQPpGCv0},
 year = {2022}
}

@article{li_functional_2023,
 author = {Li, Shanda and You, Chong and Guruganesh, Guru and Ainslie, Joshua and Ontanon, Santiago and Zaheer, Manzil and Sanghai, Sumit and Yang, Yiming and Kumar, Sanjiv and Bhojanapalli, Srinadh},
 journal = {ArXiv preprint},
 title = {Functional interpolation for relative positions improves long context transformers},
 url = {https://arxiv.org/abs/2310.04418},
 volume = {abs/2310.04418},
 year = {2023}
}

@article{loshchilov_fixing_2018,
 author = {Loshchilov, Ilya and Hutter, Frank},
 title = {Fixing weight decay regularization in adam},
 year = {2018}
}

@article{chen_extending_2023,
 author = {Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
 journal = {ArXiv preprint},
 title = {Extending context window of large language models via positional interpolation},
 url = {https://arxiv.org/abs/2306.15595},
 volume = {abs/2306.15595},
 year = {2023}
}

@inproceedings{xiong_effective_2023,
 address = {Mexico City, Mexico},
 author = {Xiong, Wenhan  and
Liu, Jingyu  and
Molybog, Igor  and
Zhang, Hejia  and
Bhargava, Prajjwal  and
Hou, Rui  and
Martin, Louis  and
Rungta, Rashi  and
Sankararaman, Karthik Abinav  and
Oguz, Barlas  and
Khabsa, Madian  and
Fang, Han  and
Mehdad, Yashar  and
Narang, Sharan  and
Malik, Kshitiz  and
Fan, Angela  and
Bhosale, Shruti  and
Edunov, Sergey  and
Lewis, Mike  and
Wang, Sinong  and
Ma, Hao},
 booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
 editor = {Duh, Kevin  and
Gomez, Helena  and
Bethard, Steven},
 pages = {4643--4663},
 publisher = {Association for Computational Linguistics},
 title = {Effective Long-Context Scaling of Foundation Models},
 url = {https://aclanthology.org/2024.naacl-long.260},
 year = {2024}
}

@inproceedings{rae_compressive_2019,
 author = {Jack W. Rae and
Anna Potapenko and
Siddhant M. Jayakumar and
Chloe Hillier and
Timothy P. Lillicrap},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/RaePJHL20.bib},
 booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
Addis Ababa, Ethiopia, April 26-30, 2020},
 publisher = {OpenReview.net},
 timestamp = {Thu, 07 May 2020 01:00:00 +0200},
 title = {Compressive Transformers for Long-Range Sequence Modelling},
 url = {https://openreview.net/forum?id=SylKikSYDH},
 year = {2020}
}

@article{jiang_mistral_2023,
 author = {Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and {others}},
 journal = {ArXiv preprint},
 title = {Mistral {7B}},
 url = {https://arxiv.org/abs/2310.06825},
 volume = {abs/2310.06825},
 year = {2023}
}

@misc{soboleva_slimpajama_2023,
 author = {Soboleva, Daria and Al-Khateeb, Faisal and Myers, Robert and Steeves, Jacob R and Hestness, Joel and Dey, Nolan},
 title = {{SlimPajama}: {A} {627B} token cleaned and deduplicated version of {RedPajama}},
 year = {2023}
}

@misc{buckman_linear_2024,
 author = {Buckman, Jacob and Gelada, Carles},
 publisher = {Manifest AI},
 title = {Linear {Transformers} {Are} {Faster} {After} {All}},
 year = {2024}
}

@misc{gao_framework_2021,
 author = {Gao, Leo and Tow, Jonathan and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and McDonell, Kyle and Muennighoff, Niklas and Phang, Jason and Reynolds, Laria and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
 doi = {10.5281/zenodo.5371628},
 publisher = {Zenodo},
 title = {A framework for few-shot language model evaluation},
 year = {2021}
}

@inproceedings{sun_length-extrapolatable_2023,
 address = {Toronto, Canada},
 author = {Sun, Yutao  and
Dong, Li  and
Patra, Barun  and
Ma, Shuming  and
Huang, Shaohan  and
Benhaim, Alon  and
Chaudhary, Vishrav  and
Song, Xia  and
Wei, Furu},
 booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2023.acl-long.816},
 editor = {Rogers, Anna  and
Boyd-Graber, Jordan  and
Okazaki, Naoaki},
 pages = {14590--14604},
 publisher = {Association for Computational Linguistics},
 title = {A Length-Extrapolatable Transformer},
 url = {https://aclanthology.org/2023.acl-long.816},
 year = {2023}
}

@article{auer_sciqa_2023,
 author = {Auer, Sören and Barone, Dante A. C. and Bartz, Cassiano and Cortes, Eduardo G. and Jaradeh, Mohamad Yaser and Karras, Oliver and Koubarakis, Manolis and Mouromtsev, Dmitry and Pliukhin, Dmitrii and Radyush, Daniil and Shilin, Ivan and Stocker, Markus and Tsalapati, Eleni},
 doi = {10.1038/s41598-023-33607-z},
 issn = {2045-2322},
 journal = {Scientific Reports},
 number = {1},
 pages = {7240},
 title = {The {SciQA} {Scientific} {Question} {Answering} {Benchmark} for {Scholarly} {Knowledge}},
 volume = {13},
 year = {2023}
}

@inproceedings{clark_boolq_2019,
 address = {Minneapolis, Minnesota},
 author = {Clark, Christopher  and
Lee, Kenton  and
Chang, Ming-Wei  and
Kwiatkowski, Tom  and
Collins, Michael  and
Toutanova, Kristina},
 booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
 doi = {10.18653/v1/N19-1300},
 editor = {Burstein, Jill  and
Doran, Christy  and
Solorio, Thamar},
 pages = {2924--2936},
 publisher = {Association for Computational Linguistics},
 title = {{B}ool{Q}: Exploring the Surprising Difficulty of Natural Yes/No Questions},
 url = {https://aclanthology.org/N19-1300},
 year = {2019}
}

@inproceedings{mihaylov_can_2018,
 address = {Brussels, Belgium},
 author = {Mihaylov, Todor  and
Clark, Peter  and
Khot, Tushar  and
Sabharwal, Ashish},
 booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/D18-1260},
 editor = {Riloff, Ellen  and
Chiang, David  and
Hockenmaier, Julia  and
Tsujii, Jun{'}ichi},
 pages = {2381--2391},
 publisher = {Association for Computational Linguistics},
 title = {Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
 url = {https://aclanthology.org/D18-1260},
 year = {2018}
}

@article{reddy_coqa_2019,
 address = {Cambridge, MA},
 author = {Reddy, Siva  and
Chen, Danqi  and
Manning, Christopher D.},
 doi = {10.1162/tacl_a_00266},
 editor = {Lee, Lillian  and
Johnson, Mark  and
Roark, Brian  and
Nenkova, Ani},
 journal = {Transactions of the Association for Computational Linguistics},
 pages = {249--266},
 publisher = {MIT Press},
 title = {{C}o{QA}: A Conversational Question Answering Challenge},
 url = {https://aclanthology.org/Q19-1016},
 volume = {7},
 year = {2019}
}

@inproceedings{sakaguchi_winogrande_2021,
 author = {Keisuke Sakaguchi and
Ronan Le Bras and
Chandra Bhagavatula and
Yejin Choi},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/aaai/SakaguchiBBC20.bib},
 booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
2020, The Thirty-Second Innovative Applications of Artificial Intelligence
Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
February 7-12, 2020},
 pages = {8732--8740},
 publisher = {{AAAI} Press},
 timestamp = {Tue, 02 Feb 2021 00:00:00 +0100},
 title = {WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
 url = {https://aaai.org/ojs/index.php/AAAI/article/view/6399},
 year = {2020}
}

@inproceedings{bisk_piqa_2020,
 author = {Yonatan Bisk and
Rowan Zellers and
Ronan LeBras and
Jianfeng Gao and
Yejin Choi},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/aaai/BiskZLGC20.bib},
 booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
2020, The Thirty-Second Innovative Applications of Artificial Intelligence
Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
February 7-12, 2020},
 pages = {7432--7439},
 publisher = {{AAAI} Press},
 timestamp = {Thu, 04 Jun 2020 01:00:00 +0200},
 title = {{PIQA:} Reasoning about Physical Commonsense in Natural Language},
 url = {https://aaai.org/ojs/index.php/AAAI/article/view/6239},
 year = {2020}
}

@inproceedings{zellers_hellaswag_2019,
 address = {Florence, Italy},
 author = {Zellers, Rowan  and
Holtzman, Ari  and
Bisk, Yonatan  and
Farhadi, Ali  and
Choi, Yejin},
 booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/P19-1472},
 editor = {Korhonen, Anna  and
Traum, David  and
M{\`a}rquez, Llu{\'\i}s},
 pages = {4791--4800},
 publisher = {Association for Computational Linguistics},
 title = {{H}ella{S}wag: Can a Machine Really Finish Your Sentence?},
 url = {https://aclanthology.org/P19-1472},
 year = {2019}
}

@article{clark_think_2018,
 author = {Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
 journal = {ArXiv preprint},
 title = {Think you have solved question answering? try arc, the ai2 reasoning challenge},
 url = {https://arxiv.org/abs/1803.05457},
 volume = {abs/1803.05457},
 year = {2018}
}


@inproceedings{zhang_root_2019,
 author = {Biao Zhang and
Rico Sennrich},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/ZhangS19a.bib},
 booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019, NeurIPS 2019, December
8-14, 2019, Vancouver, BC, Canada},
 editor = {Hanna M. Wallach and
Hugo Larochelle and
Alina Beygelzimer and
Florence d'Alch{\'{e}}{-}Buc and
Emily B. Fox and
Roman Garnett},
 pages = {12360--12371},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Root Mean Square Layer Normalization},
 url = {https://proceedings.neurips.cc/paper/2019/hash/1e8a19426224ca89e83cef47f1e7f53b-Abstract.html},
 year = {2019}
}

@inproceedings{tillet_triton_2019,
 author = {Tillet, Philippe and Kung, Hsiang-Tsung and Cox, David D.},
 booktitle = {Proceedings of the 3rd {ACM} {SIGPLAN} {International} {Workshop} on {Machine} {Learning} and {Programming} {Languages}},
 publisher = {ACM},
 title = {Triton: an intermediate language and compiler for tiled neural network computations},
 year = {2019}
}

@inproceedings{peng_random_2021,
 author = {Hao Peng and
Nikolaos Pappas and
Dani Yogatama and
Roy Schwartz and
Noah A. Smith and
Lingpeng Kong},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/Peng0Y0SK21.bib},
 booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
Virtual Event, Austria, May 3-7, 2021},
 publisher = {OpenReview.net},
 timestamp = {Wed, 23 Jun 2021 01:00:00 +0200},
 title = {Random Feature Attention},
 url = {https://openreview.net/forum?id=QtTKTdVrFBB},
 year = {2021}
}

@article{katsch_gateloop_2023,
 author = {Katsch, Tobias},
 journal = {ArXiv preprint},
 title = {{GateLoop}: {Fully} {Data}-{Controlled} {Linear} {Recurrence} for {Sequence} {Modeling}},
 url = {https://arxiv.org/abs/2311.01927},
 volume = {abs/2311.01927},
 year = {2023}
}

@inproceedings{heinsen_efficient_2023,
 author = {Heinsen, Franz A.},
 title = {Efficient {Parallelization} of an {Ubiquitous} {Sequential} {Computation}},
 year = {2023}
}

@article{touvron_llama_2023,
 author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and {others}},
 journal = {ArXiv preprint},
 title = {Llama: {Open} and efficient foundation language models},
 url = {https://arxiv.org/abs/2302.13971},
 volume = {abs/2302.13971},
 year = {2023}
}

@inproceedings{choromanski_rethinking_2020,
 author = {Krzysztof Marcin Choromanski and
Valerii Likhosherstov and
David Dohan and
Xingyou Song and
Andreea Gane and
Tam{\'{a}}s Sarl{\'{o}}s and
Peter Hawkins and
Jared Quincy Davis and
Afroz Mohiuddin and
Lukasz Kaiser and
David Benjamin Belanger and
Lucy J. Colwell and
Adrian Weller},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/ChoromanskiLDSG21.bib},
 booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
Virtual Event, Austria, May 3-7, 2021},
 publisher = {OpenReview.net},
 timestamp = {Wed, 23 Jun 2021 01:00:00 +0200},
 title = {Rethinking Attention with Performers},
 url = {https://openreview.net/forum?id=Ua6zuk0WRH},
 year = {2021}
}

@inproceedings{qin_devil_2022,
 address = {Abu Dhabi, United Arab Emirates},
 author = {Qin, Zhen  and
Han, Xiaodong  and
Sun, Weixuan  and
Li, Dongxu  and
Kong, Lingpeng  and
Barnes, Nick  and
Zhong, Yiran},
 booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2022.emnlp-main.473},
 editor = {Goldberg, Yoav  and
Kozareva, Zornitsa  and
Zhang, Yue},
 pages = {7025--7041},
 publisher = {Association for Computational Linguistics},
 title = {The Devil in Linear Transformer},
 url = {https://aclanthology.org/2022.emnlp-main.473},
 year = {2022}
}

@inproceedings{vaswani_attention_2017,
 author = {Ashish Vaswani and
Noam Shazeer and
Niki Parmar and
Jakob Uszkoreit and
Llion Jones and
Aidan N. Gomez and
Lukasz Kaiser and
Illia Polosukhin},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
 booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, {USA}},
 editor = {Isabelle Guyon and
Ulrike von Luxburg and
Samy Bengio and
Hanna M. Wallach and
Rob Fergus and
S. V. N. Vishwanathan and
Roman Garnett},
 pages = {5998--6008},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
 year = {2017}
}

@inproceedings{DBLP:conf/iclr/IrieS23,
 author = {Kazuki Irie and
J{\"{u}}rgen Schmidhuber},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/IrieS23.bib},
 booktitle = {The Eleventh International Conference on Learning Representations,
{ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
 publisher = {OpenReview.net},
 timestamp = {Fri, 30 Jun 2023 01:00:00 +0200},
 title = {Images as Weight Matrices: Sequential Image Generation Through Synaptic
Learning Rules},
 url = {https://openreview.net/pdf?id=ddad0PNUvV},
 year = {2023}
}

@inproceedings{DBLP:conf/nips/IrieFS22,
 author = {Kazuki Irie and
Francesco Faccio and
J{\"{u}}rgen Schmidhuber},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/IrieFS22.bib},
 booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022},
 editor = {Sanmi Koyejo and
S. Mohamed and
A. Agarwal and
Danielle Belgrave and
K. Cho and
A. Oh},
 timestamp = {Mon, 08 Jan 2024 00:00:00 +0100},
 title = {Neural Differential Equations for Learning to Program Neural Nets
Through Continuous Learning Rules},
 url = {http://papers.nips.cc/paper\_files/paper/2022/hash/fc09b26b85ab3abb2832bd555a2e4215-Abstract-Conference.html},
 year = {2022}
}

@article{dayan1991optimising,
 author = {Dayan, Peter and Willshaw, David J},
 journal = {Biological cybernetics},
 number = {4},
 pages = {253--265},
 publisher = {Springer},
 title = {Optimising synaptic learning rules in linear associative memories},
 volume = {65},
 year = {1991}
}

@software{yang2024fla,
 author = {Yang, Songlin and Zhang, Yu},
 journal = {https://github.com/sustcsonglin/flash-linear-attention},
 title = {FLA: A Triton-Based Library for Hardware-Efficient Implementations of Linear Attention Mechanism},
 year = {2024}
}

@article{mamba2,
 author = {Tri Dao and Albert Gu},
 journal = {arXiv preprint arXiv: 2405.21060},
 title = {Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality},
 year = {2024}
}

@article{beck2024xlstm,
 author = {Beck, Maximilian and P{\"o}ppel, Korbinian and Spanring, Markus and Auer, Andreas and Prudnikova, Oleksandra and Kopp, Michael and Klambauer, G{\"u}nter and Brandstetter, Johannes and Hochreiter, Sepp},
 journal = {ArXiv preprint},
 title = {xLSTM: Extended Long Short-Term Memory},
 url = {https://arxiv.org/abs/2405.04517},
 volume = {abs/2405.04517},
 year = {2024}
}

@inproceedings{zaheer2020big,
 author = {Manzil Zaheer and
Guru Guruganesh and
Kumar Avinava Dubey and
Joshua Ainslie and
Chris Alberti and
Santiago Onta{\~{n}}{\'{o}}n and
Philip Pham and
Anirudh Ravula and
Qifan Wang and
Li Yang and
Amr Ahmed},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/ZaheerGDAAOPRWY20.bib},
 booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
 editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
 timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
 title = {Big Bird: Transformers for Longer Sequences},
 url = {https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html},
 year = {2020}
}

@article{kang2023fast,
 author = {Kang, Yanming and Tran, Giang and De Sterck, Hans},
 journal = {ArXiv preprint},
 title = {Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences},
 url = {https://arxiv.org/abs/2310.11960},
 volume = {abs/2310.11960},
 year = {2023}
}

@inproceedings{nguyen2021fmmformer,
 author = {Tan M. Nguyen and
Vai Suliafu and
Stanley J. Osher and
Long Chen and
Bao Wang},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/NguyenSOCW21.bib},
 booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
on Neural Information Processing Systems 2021, NeurIPS 2021, December
6-14, 2021, virtual},
 editor = {Marc'Aurelio Ranzato and
Alina Beygelzimer and
Yann N. Dauphin and
Percy Liang and
Jennifer Wortman Vaughan},
 pages = {29449--29463},
 timestamp = {Tue, 03 May 2022 01:00:00 +0200},
 title = {FMMformer: Efficient and Flexible Transformer via Decomposed Near-field
and Far-field Attention},
 url = {https://proceedings.neurips.cc/paper/2021/hash/f621585df244e9596dc70a39b579efb1-Abstract.html},
 year = {2021}
}

@inproceedings{kitaev2020reformer,
 author = {Nikita Kitaev and
Lukasz Kaiser and
Anselm Levskaya},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/KitaevKL20.bib},
 booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
Addis Ababa, Ethiopia, April 26-30, 2020},
 publisher = {OpenReview.net},
 timestamp = {Thu, 07 May 2020 01:00:00 +0200},
 title = {Reformer: The Efficient Transformer},
 url = {https://openreview.net/forum?id=rkgNKkHtvB},
 year = {2020}
}

@article{roy2021efficient,
 address = {Cambridge, MA},
 author = {Roy, Aurko  and
Saffar, Mohammad  and
Vaswani, Ashish  and
Grangier, David},
 doi = {10.1162/tacl_a_00353},
 editor = {Roark, Brian  and
Nenkova, Ani},
 journal = {Transactions of the Association for Computational Linguistics},
 pages = {53--68},
 publisher = {MIT Press},
 title = {Efficient Content-Based Sparse Attention with Routing Transformers},
 url = {https://aclanthology.org/2021.tacl-1.4},
 volume = {9},
 year = {2021}
}

@inproceedings{hasani2022liquid,
 author = {Ramin M. Hasani and
Mathias Lechner and
Tsun{-}Hsuan Wang and
Makram Chahine and
Alexander Amini and
Daniela Rus},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/HasaniLWCAR23.bib},
 booktitle = {The Eleventh International Conference on Learning Representations,
{ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
 publisher = {OpenReview.net},
 timestamp = {Fri, 30 Jun 2023 01:00:00 +0200},
 title = {Liquid Structural State-Space Models},
 url = {https://openreview.net/pdf?id=g4OTKRKfS7R},
 year = {2023}
}

@inproceedings{irie-etal-2023-practical,
 address = {Singapore},
 author = {Irie, Kazuki  and
Csord{\'a}s, R{\'o}bert  and
Schmidhuber, J{\"u}rgen},
 booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2023.emnlp-main.588},
 editor = {Bouamor, Houda  and
Pino, Juan  and
Bali, Kalika},
 pages = {9455--9465},
 publisher = {Association for Computational Linguistics},
 title = {Practical Computational Power of Linear Transformers and Their Recurrent and Self-Referential Extensions},
 url = {https://aclanthology.org/2023.emnlp-main.588},
 year = {2023}
}

@article{Rodkin2024AssociativeRM,
 author = {Ivan Rodkin and Yuri Kuratov and Aydar Bulatov and Mikhail Burtsev},
 journal = {ArXiv preprint},
 title = {Associative Recurrent Memory Transformer},
 url = {https://arxiv.org/abs/2407.04841},
 volume = {abs/2407.04841},
 year = {2024}
}

@inproceedings{qin2023toeplitz,
 author = {Zhen Qin and
Xiaodong Han and
Weixuan Sun and
Bowen He and
Dong Li and
Dongxu Li and
Yuchao Dai and
Lingpeng Kong and
Yiran Zhong},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/QinHSHLLDKZ23.bib},
 booktitle = {The Eleventh International Conference on Learning Representations,
{ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
 publisher = {OpenReview.net},
 timestamp = {Fri, 01 Mar 2024 00:00:00 +0100},
 title = {Toeplitz Neural Network for Sequence Modeling},
 url = {https://openreview.net/pdf?id=IxmWsm4xrua},
 year = {2023}
}

@inproceedings{Irie2022AMS,
 author = {Kazuki Irie and
Imanol Schlag and
R{\'{o}}bert Csord{\'{a}}s and
J{\"{u}}rgen Schmidhuber},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/IrieSCS22.bib},
 booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
2022, Baltimore, Maryland, {USA}},
 editor = {Kamalika Chaudhuri and
Stefanie Jegelka and
Le Song and
Csaba Szepesv{\'{a}}ri and
Gang Niu and
Sivan Sabato},
 pages = {9660--9677},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Tue, 12 Jul 2022 01:00:00 +0200},
 title = {A Modern Self-Referential Weight Matrix That Learns to Modify Itself},
 url = {https://proceedings.mlr.press/v162/irie22b.html},
 volume = {162},
 year = {2022}
}

@article{fu2024data,
 author = {Fu, Yao and Panda, Rameswar and Niu, Xinyao and Yue, Xiang and Hajishirzi, Hannaneh and Kim, Yoon and Peng, Hao},
 journal = {ArXiv preprint},
 title = {Data Engineering for Scaling Language Models to 128K Context},
 url = {https://arxiv.org/abs/2402.10171},
 volume = {abs/2402.10171},
 year = {2024}
}

@article{chen2023longlora,
 author = {Chen, Yukang and Qian, Shengju and Tang, Haotian and Lai, Xin and Liu, Zhijian and Han, Song and Jia, Jiaya},
 journal = {ArXiv preprint},
 title = {Longlora: Efficient fine-tuning of long-context large language models},
 url = {https://arxiv.org/abs/2309.12307},
 volume = {abs/2309.12307},
 year = {2023}
}

@article{munkhdalai2024leave,
 author = {Munkhdalai, Tsendsuren and Faruqui, Manaal and Gopal, Siddharth},
 journal = {ArXiv preprint},
 title = {Leave no context behind: Efficient infinite context transformers with infini-attention},
 url = {https://arxiv.org/abs/2404.07143},
 volume = {abs/2404.07143},
 year = {2024}
}

@article{yen2024long,
 author = {Yen, Howard and Gao, Tianyu and Chen, Danqi},
 journal = {ArXiv preprint},
 title = {Long-Context Language Modeling with Parallel Context Encoding},
 url = {https://arxiv.org/abs/2402.16617},
 volume = {abs/2402.16617},
 year = {2024}
}

@article{liu2023ring,
 author = {Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
 journal = {ArXiv preprint},
 title = {Ring attention with blockwise transformers for near-infinite context},
 url = {https://arxiv.org/abs/2310.01889},
 volume = {abs/2310.01889},
 year = {2023}
}

@article{liu2024world,
 author = {Liu, Hao and Yan, Wilson and Zaharia, Matei and Abbeel, Pieter},
 journal = {ArXiv preprint},
 title = {World Model on Million-Length Video And Language With RingAttention},
 url = {https://arxiv.org/abs/2402.08268},
 volume = {abs/2402.08268},
 year = {2024}
}

@inproceedings{bahdanau2014neural,
 author = {Dzmitry Bahdanau and
Kyunghyun Cho and
Yoshua Bengio},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
 booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
 editor = {Yoshua Bengio and
Yann LeCun},
 timestamp = {Fri, 29 Mar 2019 00:00:00 +0100},
 title = {Neural Machine Translation by Jointly Learning to Align and Translate},
 url = {http://arxiv.org/abs/1409.0473},
 year = {2015}
}

@article{yang2024vivim,
 author = {Yang, Yijun and Xing, Zhaohu and Zhu, Lei},
 journal = {ArXiv preprint},
 title = {Vivim: a Video Vision Mamba for Medical Video Object Segmentation},
 url = {https://arxiv.org/abs/2401.14168},
 volume = {abs/2401.14168},
 year = {2024}
}

@article{ma2024u,
 author = {Ma, Jun and Li, Feifei and Wang, Bo},
 journal = {ArXiv preprint},
 title = {U-mamba: Enhancing long-range dependency for biomedical image segmentation},
 url = {https://arxiv.org/abs/2401.04722},
 volume = {abs/2401.04722},
 year = {2024}
}

@article{zhu2024vision,
 author = {Zhu, Lianghui and Liao, Bencheng and Zhang, Qian and Wang, Xinlong and Liu, Wenyu and Wang, Xinggang},
 journal = {ArXiv preprint},
 title = {Vision mamba: Efficient visual representation learning with bidirectional state space model},
 url = {https://arxiv.org/abs/2401.09417},
 volume = {abs/2401.09417},
 year = {2024}
}

@article{liu2024vmamba,
 author = {Liu, Yue and Tian, Yunjie and Zhao, Yuzhong and Yu, Hongtian and Xie, Lingxi and Wang, Yaowei and Ye, Qixiang and Liu, Yunfan},
 journal = {ArXiv preprint},
 title = {Vmamba: Visual state space model},
 url = {https://arxiv.org/abs/2401.10166},
 volume = {abs/2401.10166},
 year = {2024}
}

@article{xing2024segmamba,
 author = {Xing, Zhaohu and Ye, Tian and Yang, Yijun and Liu, Guang and Zhu, Lei},
 journal = {ArXiv preprint},
 title = {Segmamba: Long-range sequential modeling mamba for 3d medical image segmentation},
 url = {https://arxiv.org/abs/2401.13560},
 volume = {abs/2401.13560},
 year = {2024}
}

@article{wang2024mambabyte,
 author = {Wang, Junxiong and Gangavarapu, Tushaar and Yan, Jing Nathan and Rush, Alexander M},
 journal = {ArXiv preprint},
 title = {MambaByte: Token-free Selective State Space Model},
 url = {https://arxiv.org/abs/2401.13660},
 volume = {abs/2401.13660},
 year = {2024}
}

@article{wang2024graph,
 author = {Wang, Chloe and Tsepa, Oleksii and Ma, Jun and Wang, Bo},
 journal = {ArXiv preprint},
 title = {Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective State Spaces},
 url = {https://arxiv.org/abs/2402.00789},
 volume = {abs/2402.00789},
 year = {2024}
}

@article{Oren2024TransformersAM,
 author = {Matanel Oren and Michael Hassid and Yossi Adi and Roy Schwartz},
 journal = {ArXiv preprint},
 title = {Transformers are Multi-State RNNs},
 url = {https://arxiv.org/abs/2401.06104},
 volume = {abs/2401.06104},
 year = {2024}
}

@article{DBLP:journals/corr/abs-2310-20051,
 author = {Zhao Song and
Guangyi Xu and
Junze Yin},
 journal = {ArXiv preprint},
 title = {The Expressibility of Polynomial based Attention Scheme},
 url = {https://arxiv.org/abs/2310.20051},
 volume = {abs/2310.20051},
 year = {2023}
}
@misc{grazzi2024unlockingstatetrackinglinearrnns,
      title={Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues}, 
      author={Riccardo Grazzi and Julien Siems and Jörg K. H. Franke and Arber Zela and Frank Hutter and Massimiliano Pontil},
      year={2024},
      eprint={2411.12537},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.12537}, 
}
@article{Shen2024PowerSA,
  title={Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler},
  author={Yikang Shen and Matt Stallone and Mayank Mishra and Gaoyuan Zhang and Shawn Tan and Aditya Prasad and Adriana Meza Soria and David D. Cox and Rameswar Panda},
  journal={ArXiv},
  year={2024},
  volume={abs/2408.13359},
  url={https://api.semanticscholar.org/CorpusID:271957320}
}
@inproceedings{cosformer,
 author = {Zhen Qin and
Weixuan Sun and
Hui Deng and
Dongxu Li and
Yunshen Wei and
Baohong Lv and
Junjie Yan and
Lingpeng Kong and
Yiran Zhong},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/QinSDLWLYKZ22.bib},
 booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
 publisher = {OpenReview.net},
 timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
 title = {cosFormer: Rethinking Softmax In Attention},
 url = {https://openreview.net/forum?id=Bl8CQrx2Up4},
 year = {2022}
}

@misc{Hedgehog,
 author = {Michael Zhang and Kush Bhatia and Hermann Kumbong and Christopher Ré},
 journal = {ArXiv preprint},
 title = {The Hedgehog \& the Porcupine: Expressive Linear Attentions with Softmax Mimicry},
 url = {https://arxiv.org/abs/2402.04347},
 volume = {abs/2402.04347},
 year = {2024}
}

@inproceedings{Alman2023FastAR,
 author = {Josh Alman and
Zhao Song},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/Alman023.bib},
 booktitle = {Advances in Neural Information Processing Systems 36: Annual Conference
on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023},
 editor = {Alice Oh and
Tristan Naumann and
Amir Globerson and
Kate Saenko and
Moritz Hardt and
Sergey Levine},
 timestamp = {Fri, 01 Mar 2024 00:00:00 +0100},
 title = {Fast Attention Requires Bounded Entries},
 url = {http://papers.nips.cc/paper\_files/paper/2023/hash/c72861451d6fa9dfa64831102b9bb71a-Abstract-Conference.html},
 year = {2023}
}

@misc{orthogonal_memory,
 author = {Jun Zhang and Shuyang Jiang and Jiangtao Feng and Lin Zheng and Lingpeng Kong},
 journal = {ArXiv preprint},
 title = {Linear Attention via Orthogonal Memory},
 url = {https://arxiv.org/abs/2312.11135},
 volume = {abs/2312.11135},
 year = {2023}
}

@inproceedings{zhang-cai-2022-linearizing,
 address = {Abu Dhabi, United Arab Emirates},
 author = {Zhang, Yizhe  and
Cai, Deng},
 booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2022.emnlp-main.24},
 editor = {Goldberg, Yoav  and
Kozareva, Zornitsa  and
Zhang, Yue},
 pages = {346--359},
 publisher = {Association for Computational Linguistics},
 title = {Linearizing Transformer with Key-Value Memory},
 url = {https://aclanthology.org/2022.emnlp-main.24},
 year = {2022}
}

@inproceedings{Massaroli2023LaughingHD,
 author = {Stefano Massaroli and
Michael Poli and
Daniel Y. Fu and
Hermann Kumbong and
Rom N. Parnichkun and
David W. Romero and
Aman Timalsina and
Quinn McIntyre and
Beidi Chen and
Atri Rudra and
Ce Zhang and
Christopher R{\'{e}} and
Stefano Ermon and
Yoshua Bengio},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/MassaroliPFKPRT23.bib},
 booktitle = {Advances in Neural Information Processing Systems 36: Annual Conference
on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023},
 editor = {Alice Oh and
Tristan Naumann and
Amir Globerson and
Kate Saenko and
Moritz Hardt and
Sergey Levine},
 timestamp = {Fri, 01 Mar 2024 00:00:00 +0100},
 title = {Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions},
 url = {http://papers.nips.cc/paper\_files/paper/2023/hash/371355cd42caaf83412c3fbef4688979-Abstract-Conference.html},
 year = {2023}
}

@inproceedings{peng-etal-2022-abc,
 address = {Dublin, Ireland},
 author = {Peng, Hao  and
Kasai, Jungo  and
Pappas, Nikolaos  and
Yogatama, Dani  and
Wu, Zhaofeng  and
Kong, Lingpeng  and
Schwartz, Roy  and
Smith, Noah A.},
 booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2022.acl-long.515},
 editor = {Muresan, Smaranda  and
Nakov, Preslav  and
Villavicencio, Aline},
 pages = {7469--7483},
 publisher = {Association for Computational Linguistics},
 title = {{ABC}: Attention with Bounded-memory Control},
 url = {https://aclanthology.org/2022.acl-long.515},
 year = {2022}
}

@article{polysketch,
 author = {Kacham, Praneeth and Mirrokni, Vahab and Zhong, Peilin},
 journal = {ArXiv preprint},
 title = {Polysketchformer: Fast transformers via sketches for polynomial kernels},
 url = {https://arxiv.org/abs/2310.01655},
 volume = {abs/2310.01655},
 year = {2023}
}

@inproceedings{hyena,
 author = {Michael Poli and
Stefano Massaroli and
Eric Nguyen and
Daniel Y. Fu and
Tri Dao and
Stephen Baccus and
Yoshua Bengio and
Stefano Ermon and
Christopher R{\'{e}}},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/PoliMNFDBBER23.bib},
 booktitle = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
2023, Honolulu, Hawaii, {USA}},
 editor = {Andreas Krause and
Emma Brunskill and
Kyunghyun Cho and
Barbara Engelhardt and
Sivan Sabato and
Jonathan Scarlett},
 pages = {28043--28078},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Mon, 28 Aug 2023 01:00:00 +0200},
 title = {Hyena Hierarchy: Towards Larger Convolutional Language Models},
 url = {https://proceedings.mlr.press/v202/poli23a.html},
 volume = {202},
 year = {2023}
}

@inproceedings{lightning2,
 author = {Zhen Qin and Weigao Sun and Dong Li and Xuyang Shen and Weixuan Sun and Yiran Zhong},
 title = {Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models},
 year = {2024}
}

@inproceedings{h3,
 author = {Daniel Y. Fu and
Tri Dao and
Khaled Kamal Saab and
Armin W. Thomas and
Atri Rudra and
Christopher R{\'{e}}},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/FuDSTRR23.bib},
 booktitle = {The Eleventh International Conference on Learning Representations,
{ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
 publisher = {OpenReview.net},
 timestamp = {Fri, 30 Jun 2023 01:00:00 +0200},
 title = {Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
 url = {https://openreview.net/pdf?id=COZDy0WYGg},
 year = {2023}
}

@article{zoology,
 author = {Simran Arora and
Sabri Eyuboglu and
Aman Timalsina and
Isys Johnson and
Michael Poli and
James Zou and
Atri Rudra and
Christopher R{\'{e}}},
 journal = {ArXiv preprint},
 title = {Zoology: Measuring and Improving Recall in Efficient Language Models},
 url = {https://arxiv.org/abs/2312.04927},
 volume = {abs/2312.04927},
 year = {2023}
}

@article{Blelloch1990PrefixSA,
 author = {Blelloch, Guy E},
 publisher = {School of Computer Science, Carnegie Mellon University Pittsburgh, PA, USA},
 title = {Prefix sums and their applications},
 year = {1990}
}

@inproceedings{s5,
 author = {Jimmy T. H. Smith and
Andrew Warrington and
Scott W. Linderman},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/SmithWL23.bib},
 booktitle = {The Eleventh International Conference on Learning Representations,
{ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
 publisher = {OpenReview.net},
 timestamp = {Fri, 30 Jun 2023 01:00:00 +0200},
 title = {Simplified State Space Layers for Sequence Modeling},
 url = {https://openreview.net/pdf?id=Ai8Hw3AXqks},
 year = {2023}
}

@inproceedings{s4,
 author = {Albert Gu and
Karan Goel and
Christopher R{\'{e}}},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/GuGR22.bib},
 booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
 publisher = {OpenReview.net},
 timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
 title = {Efficiently Modeling Long Sequences with Structured State Spaces},
 url = {https://openreview.net/forum?id=uYLFoz1vlAC},
 year = {2022}
}

@article{DBLP:journals/corr/GravesWD14,
  author       = {Alex Graves and
                  Greg Wayne and
                  Ivo Danihelka},
  title        = {Neural Turing Machines},
  journal      = {CoRR},
  volume       = {abs/1410.5401},
  year         = {2014},
  url          = {http://arxiv.org/abs/1410.5401},
  eprinttype    = {arXiv},
  eprint       = {1410.5401},
  timestamp    = {Mon, 13 Aug 2018 16:46:28 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/GravesWD14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{peng-etal-2023-rwkv,
    title = "{RWKV}: Reinventing {RNN}s for the Transformer Era",
    author = "Peng, Bo  and
      Alcaide, Eric  and
      Anthony, Quentin  and
      Albalak, Alon  and
      Arcadinho, Samuel  and
      Biderman, Stella  and
      Cao, Huanqi  and
      Cheng, Xin  and
      Chung, Michael  and
      Derczynski, Leon  and
      Du, Xingjian  and
      Grella, Matteo  and
      Gv, Kranthi  and
      He, Xuzheng  and
      Hou, Haowen  and
      Kazienko, Przemyslaw  and
      Kocon, Jan  and
      Kong, Jiaming  and
      Koptyra, Bart{\l}omiej  and
      Lau, Hayden  and
      Lin, Jiaju  and
      Mantri, Krishna Sri Ipsit  and
      Mom, Ferdinand  and
      Saito, Atsushi  and
      Song, Guangyu  and
      Tang, Xiangru  and
      Wind, Johan  and
      Wo{\'z}niak, Stanis{\l}aw  and
      Zhang, Zhenyuan  and
      Zhou, Qinghua  and
      Zhu, Jian  and
      Zhu, Rui-Jie",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.936",
    doi = "10.18653/v1/2023.findings-emnlp.936",
    pages = "14048--14077",
    abstract = "Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.",
}

@article{recurrent_linear_xfmr,
 author = {Subhojeet Pramanik and
Esraa Elelimy and
Marlos C. Machado and
Adam White},
 journal = {ArXiv preprint},
 title = {Recurrent Linear Transformers},
 url = {https://arxiv.org/abs/2310.15719},
 volume = {abs/2310.15719},
 year = {2023}
}

@article{DBLP:journals/neco/GersSC00,
 author = {Felix A. Gers and
J{\"{u}}rgen Schmidhuber and
Fred A. Cummins},
 journal = {Neural Comput.},
 number = {10},
 pages = {2451--2471},
 title = {Learning to Forget: Continual Prediction with {LSTM}},
 volume = {12},
 year = {2000}
}

@inproceedings{Saphra2023FirstTT,
 address = {Mexico City, Mexico},
 author = {Saphra, Naomi  and
Fleisig, Eve  and
Cho, Kyunghyun  and
Lopez, Adam},
 booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
 editor = {Duh, Kevin  and
Gomez, Helena  and
Bethard, Steven},
 pages = {2310--2326},
 publisher = {Association for Computational Linguistics},
 title = {First Tragedy, then Parse: History Repeats Itself in the New Era of Large Language Models},
 url = {https://aclanthology.org/2024.naacl-long.128},
 year = {2024}
}

@article{Li2023LightSeqSL,
 author = {Dacheng Li and Rulin Shao and Anze Xie and Eric P. Xing and Joseph E. Gonzalez and Ion Stoica and Xuezhe Ma and Hao Zhang},
 journal = {ArXiv preprint},
 title = {LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers},
 url = {https://arxiv.org/abs/2310.03294},
 volume = {abs/2310.03294},
 year = {2023}
}

@inproceedings{Yan2023DiffusionMW,
 author = {Jing Nathan Yan and Jiatao Gu and Alexander M. Rush},
 title = {Diffusion Models Without Attention},
 year = {2023}
}

@article{Brandon2023StripedAF,
 author = {William Brandon and Aniruddha Nrusimha and Kevin Qian and Zachary Ankner and Tian Jin and Zhiye Song and Jonathan Ragan-Kelley},
 journal = {ArXiv preprint},
 title = {Striped Attention: Faster Ring Attention for Causal Transformers},
 url = {https://arxiv.org/abs/2311.09431},
 volume = {abs/2311.09431},
 year = {2023}
}

@article{Liu2023RingAW,
 author = {Hao Liu and Matei Zaharia and Pieter Abbeel},
 journal = {ArXiv preprint},
 title = {Ring Attention with Blockwise Transformers for Near-Infinite Context},
 url = {https://arxiv.org/abs/2310.01889},
 volume = {abs/2310.01889},
 year = {2023}
}

@inproceedings{Chaurasia2015CompilingHP,
 author = {Gaurav Chaurasia and Jonathan Ragan-Kelley and Sylvain Paris and George Drettakis and Fr{\'e}do Durand},
 booktitle = {High Performance Graphics},
 title = {Compiling high performance recursive filters},
 year = {2015}
}

@inproceedings{Fu2023MonarchMA,
 author = {Daniel Y. Fu and
Simran Arora and
Jessica Grogan and
Isys Johnson and
Evan Sabri Eyuboglu and
Armin W. Thomas and
Benjamin Spector and
Michael Poli and
Atri Rudra and
Christopher R{\'{e}}},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/FuAGJETSPRR23.bib},
 booktitle = {Advances in Neural Information Processing Systems 36: Annual Conference
on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023},
 editor = {Alice Oh and
Tristan Naumann and
Amir Globerson and
Kate Saenko and
Moritz Hardt and
Sergey Levine},
 timestamp = {Fri, 01 Mar 2024 00:00:00 +0100},
 title = {Monarch Mixer: {A} Simple Sub-Quadratic GEMM-Based Architecture},
 url = {http://papers.nips.cc/paper\_files/paper/2023/hash/f498c1ce6bff52eb04febf87438dd84b-Abstract-Conference.html},
 year = {2023}
}

@inproceedings{Dao2022MonarchES,
 author = {Tri Dao and
Beidi Chen and
Nimit Sharad Sohoni and
Arjun D. Desai and
Michael Poli and
Jessica Grogan and
Alexander Liu and
Aniruddh Rao and
Atri Rudra and
Christopher R{\'{e}}},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/DaoCSDPGLRRR22.bib},
 booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
2022, Baltimore, Maryland, {USA}},
 editor = {Kamalika Chaudhuri and
Stefanie Jegelka and
Le Song and
Csaba Szepesv{\'{a}}ri and
Gang Niu and
Sivan Sabato},
 pages = {4690--4721},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Tue, 12 Jul 2022 01:00:00 +0200},
 title = {Monarch: Expressive Structured Matrices for Efficient and Accurate
Training},
 url = {https://proceedings.mlr.press/v162/dao22a.html},
 volume = {162},
 year = {2022}
}

@article{Hooker2020TheHL,
 author = {Sara Hooker},
 journal = {Communications of the ACM},
 pages = {58 - 65},
 title = {The hardware lottery},
 volume = {64},
 year = {2020}
}

@inproceedings{li-etal-2023-sequence,
 address = {Toronto, Canada},
 author = {Li, Shenggui  and
Xue, Fuzhao  and
Baranwal, Chaitanya  and
Li, Yongbin  and
You, Yang},
 booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2023.acl-long.134},
 editor = {Rogers, Anna  and
Boyd-Graber, Jordan  and
Okazaki, Naoaki},
 pages = {2391--2404},
 publisher = {Association for Computational Linguistics},
 title = {Sequence Parallelism: Long Sequence Training from System Perspective},
 url = {https://aclanthology.org/2023.acl-long.134},
 year = {2023}
}

@inproceedings{Gu2023MambaLS,
 author = {Albert Gu and Tri Dao},
 title = {Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
 year = {2023}
}

@inproceedings{9365803,
 author = {Choquette, Jack and Lee, Edward and Krashinsky, Ronny and Balan, Vishnu and Khailany, Brucek},
 booktitle = {2021 IEEE International Solid-State Circuits Conference (ISSCC)},
 doi = {10.1109/ISSCC42613.2021.9365803},
 number = {},
 pages = {48-50},
 title = {3.2 The A100 Datacenter GPU and Ampere Architecture},
 volume = {64},
 year = {2021}
}

@article{HochSchm97,
 author = {Sepp Hochreiter and Jürgen Schmidhuber},
 journal = {Neural Computation},
 number = {8},
 optdoi = {10.1162/neco.1997.9.8.1735},
 pages = {1735--1780},
 title = {Long Short-Term Memory},
 volume = {9},
 year = {1997}
}

@inproceedings{cho2014learning,
 address = {Doha, Qatar},
 author = {Cho, Kyunghyun  and
van Merri{\"e}nboer, Bart  and
Gulcehre, Caglar  and
Bahdanau, Dzmitry  and
Bougares, Fethi  and
Schwenk, Holger  and
Bengio, Yoshua},
 booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
 doi = {10.3115/v1/D14-1179},
 editor = {Moschitti, Alessandro  and
Pang, Bo  and
Daelemans, Walter},
 pages = {1724--1734},
 publisher = {Association for Computational Linguistics},
 title = {Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation},
 url = {https://aclanthology.org/D14-1179},
 year = {2014}
}

@article{tcfft,
 author = {Bin{-}Rui Li and
Shenggan Cheng and
James Lin},
 journal = {ArXiv preprint},
 title = {tcFFT: Accelerating Half-Precision {FFT} through Tensor Cores},
 url = {https://arxiv.org/abs/2104.11471},
 volume = {abs/2104.11471},
 year = {2021}
}

@article{flashconvfft,
 author = {Daniel Y. Fu and
Hermann Kumbong and
Eric Nguyen and
Christopher R{\'{e}}},
 journal = {ArXiv preprint},
 title = {FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor
Cores},
 url = {https://arxiv.org/abs/2311.05908},
 volume = {abs/2311.05908},
 year = {2023}
}

@inproceedings{DBLP:conf/ipps/PishaL21,
 author = {Louis Pisha and
Lukasz Ligowski},
 booktitle = {35th {IEEE} International Parallel and Distributed Processing Symposium,
{IPDPS} 2021, Portland, OR, USA, May 17-21, 2021},
 pages = {507--516},
 publisher = {{IEEE}},
 title = {Accelerating non-power-of-2 size Fourier transforms with {GPU} Tensor
Cores},
 year = {2021}
}

@inproceedings{DBLP:conf/ics/DakkakLXGH19,
 author = {Abdul Dakkak and
Cheng Li and
Jinjun Xiong and
Isaac Gelado and
Wen{-}Mei W. Hwu},
 booktitle = {Proceedings of the {ACM} International Conference on Supercomputing,
{ICS} 2019, Phoenix, AZ, USA, June 26-28, 2019},
 editor = {Rudolf Eigenmann and
Chen Ding and
Sally A. McKee},
 pages = {46--57},
 publisher = {{ACM}},
 title = {Accelerating reduction and scan using tensor core units},
 year = {2019}
}

@inproceedings{pretrain_wo_attn,
 address = {Singapore},
 author = {Wang, Junxiong  and
Yan, Jing Nathan  and
Gu, Albert  and
Rush, Alexander},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
 doi = {10.18653/v1/2023.findings-emnlp.5},
 editor = {Bouamor, Houda  and
Pino, Juan  and
Bali, Kalika},
 pages = {58--69},
 publisher = {Association for Computational Linguistics},
 title = {Pretraining Without Attention},
 url = {https://aclanthology.org/2023.findings-emnlp.5},
 year = {2023}
}

@inproceedings{gss,
 author = {Harsh Mehta and
Ankit Gupta and
Ashok Cutkosky and
Behnam Neyshabur},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/Mehta0CN23.bib},
 booktitle = {The Eleventh International Conference on Learning Representations,
{ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
 publisher = {OpenReview.net},
 timestamp = {Fri, 30 Jun 2023 01:00:00 +0200},
 title = {Long Range Language Modeling via Gated State Spaces},
 url = {https://openreview.net/pdf?id=5MkYIYCbva},
 year = {2023}
}

@article{swish,
 author = {Prajit Ramachandran and Barret Zoph and Quoc V. Le},
 journal = {arXiv: Neural and Evolutionary Computing},
 title = {Swish: a Self-Gated Activation Function},
 year = {2017}
}

@article{shazeer2020glu,
 author = {Shazeer, Noam},
 journal = {ArXiv preprint},
 title = {Glu variants improve transformer},
 url = {https://arxiv.org/abs/2002.05202},
 volume = {abs/2002.05202},
 year = {2020}
}

@article{rope,
 author = {Jianlin Su and
Yu Lu and
Shengfeng Pan and
Bo Wen and
Yunfeng Liu},
 journal = {ArXiv preprint},
 title = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
 url = {https://arxiv.org/abs/2104.09864},
 volume = {abs/2104.09864},
 year = {2021}
}

@inproceedings{ft_xfmr_into_rnn,
 address = {Online and Punta Cana, Dominican Republic},
 author = {Kasai, Jungo  and
Peng, Hao  and
Zhang, Yizhe  and
Yogatama, Dani  and
Ilharco, Gabriel  and
Pappas, Nikolaos  and
Mao, Yi  and
Chen, Weizhu  and
Smith, Noah A.},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2021.emnlp-main.830},
 editor = {Moens, Marie-Francine  and
Huang, Xuanjing  and
Specia, Lucia  and
Yih, Scott Wen-tau},
 pages = {10630--10643},
 publisher = {Association for Computational Linguistics},
 title = {Finetuning Pretrained Transformers into {RNN}s},
 url = {https://aclanthology.org/2021.emnlp-main.830},
 year = {2021}
}

@article{flashattention2,
 author = {Tri Dao},
 journal = {ArXiv preprint},
 title = {FlashAttention-2: Faster Attention with Better Parallelism and Work
Partitioning},
 url = {https://arxiv.org/abs/2307.08691},
 volume = {abs/2307.08691},
 year = {2023}
}

@inproceedings{flashattention1,
 author = {Tri Dao and
Daniel Y. Fu and
Stefano Ermon and
Atri Rudra and
Christopher R{\'{e}}},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/DaoFERR22.bib},
 booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022},
 editor = {Sanmi Koyejo and
S. Mohamed and
A. Agarwal and
Danielle Belgrave and
K. Cho and
A. Oh},
 timestamp = {Mon, 08 Jan 2024 00:00:00 +0100},
 title = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
 url = {http://papers.nips.cc/paper\_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html},
 year = {2022}
}

@inproceedings{katharopoulos2020transformers,
 author = {Angelos Katharopoulos and
Apoorv Vyas and
Nikolaos Pappas and
Fran{\c{c}}ois Fleuret},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/KatharopoulosV020.bib},
 booktitle = {Proceedings of the 37th International Conference on Machine Learning,
{ICML} 2020, 13-18 July 2020, Virtual Event},
 pages = {5156--5165},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Tue, 15 Dec 2020 00:00:00 +0100},
 title = {Transformers are RNNs: Fast Autoregressive Transformers with Linear
Attention},
 url = {http://proceedings.mlr.press/v119/katharopoulos20a.html},
 volume = {119},
 year = {2020}
}

@article{sun2023retentive,
 author = {Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu},
 journal = {ArXiv preprint},
 title = {Retentive network: A successor to transformer for large language models},
 url = {https://arxiv.org/abs/2307.08621},
 volume = {abs/2307.08621},
 year = {2023}
}

@article{qin2023scaling,
 author = {Qin, Zhen and Li, Dong and Sun, Weigao and Sun, Weixuan and Shen, Xuyang and Han, Xiaodong and Wei, Yunshen and Lv, Baohong and Yuan, Fei and Luo, Xiao and others},
 journal = {ArXiv preprint},
 title = {Scaling transnormer to 175 billion parameters},
 url = {https://arxiv.org/abs/2307.14995},
 volume = {abs/2307.14995},
 year = {2023}
}

@inproceedings{HGRN,
 author = {Zhen Qin and
Songlin Yang and
Yiran Zhong},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/QinYZ23.bib},
 booktitle = {Advances in Neural Information Processing Systems 36: Annual Conference
on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023},
 editor = {Alice Oh and
Tristan Naumann and
Amir Globerson and
Kate Saenko and
Moritz Hardt and
Sergey Levine},
 timestamp = {Fri, 01 Mar 2024 00:00:00 +0100},
 title = {Hierarchically Gated Recurrent Neural Network for Sequence Modeling},
 url = {http://papers.nips.cc/paper\_files/paper/2023/hash/694be3548697e9cc8999d45e8d16fe1e-Abstract-Conference.html},
 year = {2023}
}

@article{VQ-Transformer,
 author = {Lingle, Lucas D},
 journal = {ArXiv preprint},
 title = {Transformer-vq: Linear-time transformers via vector quantization},
 url = {https://arxiv.org/abs/2309.16354},
 volume = {abs/2309.16354},
 year = {2023}
}

@inproceedings{GAU,
 author = {Weizhe Hua and
Zihang Dai and
Hanxiao Liu and
Quoc V. Le},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/HuaDLL22.bib},
 booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
2022, Baltimore, Maryland, {USA}},
 editor = {Kamalika Chaudhuri and
Stefanie Jegelka and
Le Song and
Csaba Szepesv{\'{a}}ri and
Gang Niu and
Sivan Sabato},
 pages = {9099--9117},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Tue, 12 Jul 2022 01:00:00 +0200},
 title = {Transformer Quality in Linear Time},
 url = {https://proceedings.mlr.press/v162/hua22a.html},
 volume = {162},
 year = {2022}
}

@inproceedings{parallel-martin,
 author = {Eric Martin and
Chris Cundy},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/MartinC18.bib},
 booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
 publisher = {OpenReview.net},
 timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
 title = {Parallelizing Linear Recurrent Neural Nets Over Sequence Length},
 url = {https://openreview.net/forum?id=HyUNwulC-},
 year = {2018}
}

@inproceedings{mao-2022-fine,
 address = {Abu Dhabi, United Arab Emirates},
 author = {Mao, Huanru Henry},
 booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2022.emnlp-main.697},
 editor = {Goldberg, Yoav  and
Kozareva, Zornitsa  and
Zhang, Yue},
 pages = {10236--10242},
 publisher = {Association for Computational Linguistics},
 title = {Fine-Tuning Pre-trained Transformers into Decaying Fast Weights},
 url = {https://aclanthology.org/2022.emnlp-main.697},
 year = {2022}
}

@article{unreasonable-forget-gate,
 author = {Jos van der Westhuizen and
Joan Lasenby},
 journal = {ArXiv preprint},
 title = {The unreasonable effectiveness of the forget gate},
 url = {https://arxiv.org/abs/1804.04849},
 volume = {abs/1804.04849},
 year = {2018}
}


@inproceedings{irie2021going,
 author = {Kazuki Irie and
Imanol Schlag and
R{\'{o}}bert Csord{\'{a}}s and
J{\"{u}}rgen Schmidhuber},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/IrieSCS21.bib},
 booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
on Neural Information Processing Systems 2021, NeurIPS 2021, December
6-14, 2021, virtual},
 editor = {Marc'Aurelio Ranzato and
Alina Beygelzimer and
Yann N. Dauphin and
Percy Liang and
Jennifer Wortman Vaughan},
 pages = {7703--7717},
 timestamp = {Tue, 03 May 2022 01:00:00 +0200},
 title = {Going Beyond Linear Transformers with Recurrent Fast Weight Programmers},
 url = {https://proceedings.neurips.cc/paper/2021/hash/3f9e3767ef3b10a0de4c256d7ef9805d-Abstract.html},
 year = {2021}
}

@inproceedings{hinton1987using,
 author = {Hinton, Geoffrey E and Plaut, David C},
 booktitle = {Proceedings of the ninth annual conference of the Cognitive Science Society},
 pages = {177--186},
 title = {Using fast weights to deblur old memories},
 year = {1987}
}

@article{schmidhuber1992learning,
 author = {Schmidhuber, J{\"u}rgen},
 journal = {Neural Computation},
 number = {1},
 pages = {131--139},
 publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…},
 title = {Learning to control fast-weight memories: An alternative to dynamic recurrent networks},
 volume = {4},
 year = {1992}
}
@article{DBLP:journals/corr/abs-2406-07887,
  author       = {Roger Waleffe and
                  Wonmin Byeon and
                  Duncan Riach and
                  Brandon Norick and
                  Vijay Korthikanti and
                  Tri Dao and
                  Albert Gu and
                  Ali Hatamizadeh and
                  Sudhakar Singh and
                  Deepak Narayanan and
                  Garvit Kulshreshtha and
                  Vartika Singh and
                  Jared Casper and
                  Jan Kautz and
                  Mohammad Shoeybi and
                  Bryan Catanzaro},
  title        = {An Empirical Study of Mamba-based Language Models},
  journal      = {CoRR},
  volume       = {abs/2406.07887},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2406.07887},
  doi          = {10.48550/ARXIV.2406.07887},
  eprinttype    = {arXiv},
  eprint       = {2406.07887},
  timestamp    = {Fri, 12 Jul 2024 15:54:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2406-07887.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{ba2016using,
 author = {Jimmy Ba and
Geoffrey E. Hinton and
Volodymyr Mnih and
Joel Z. Leibo and
Catalin Ionescu},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/BaHMLI16.bib},
 booktitle = {Advances in Neural Information Processing Systems 29: Annual Conference
on Neural Information Processing Systems 2016, December 5-10, 2016,
Barcelona, Spain},
 editor = {Daniel D. Lee and
Masashi Sugiyama and
Ulrike von Luxburg and
Isabelle Guyon and
Roman Garnett},
 pages = {4331--4339},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Using Fast Weights to Attend to the Recent Past},
 url = {https://proceedings.neurips.cc/paper/2016/hash/9f44e956e3a2b7b5598c625fcc802c36-Abstract.html},
 year = {2016}
}

@inproceedings{Heinsen2023EfficientPO,
 author = {Franz A. Heinsen},
 title = {Efficient Parallelization of an Ubiquitous Sequential Computation},
 year = {2023}
}

@article{gatedloop,
 author = {Tobias Katsch},
 journal = {ArXiv preprint},
 title = {GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling},
 url = {https://arxiv.org/abs/2311.01927},
 volume = {abs/2311.01927},
 year = {2023}
}

@inproceedings{vaswani2017attention,
 author = {Ashish Vaswani and
Noam Shazeer and
Niki Parmar and
Jakob Uszkoreit and
Llion Jones and
Aidan N. Gomez and
Lukasz Kaiser and
Illia Polosukhin},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
 booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, {USA}},
 editor = {Isabelle Guyon and
Ulrike von Luxburg and
Samy Bengio and
Hanna M. Wallach and
Rob Fergus and
S. V. N. Vishwanathan and
Roman Garnett},
 pages = {5998--6008},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
 year = {2017}
}

@inproceedings{qin2022devil,
 address = {Abu Dhabi, United Arab Emirates},
 author = {Qin, Zhen  and
Han, Xiaodong  and
Sun, Weixuan  and
Li, Dongxu  and
Kong, Lingpeng  and
Barnes, Nick  and
Zhong, Yiran},
 booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2022.emnlp-main.473},
 editor = {Goldberg, Yoav  and
Kozareva, Zornitsa  and
Zhang, Yue},
 pages = {7025--7041},
 publisher = {Association for Computational Linguistics},
 title = {The Devil in Linear Transformer},
 url = {https://aclanthology.org/2022.emnlp-main.473},
 year = {2022}
}

@inproceedings{kasai-etal-2021-finetuning,
 address = {Online and Punta Cana, Dominican Republic},
 author = {Kasai, Jungo  and
Peng, Hao  and
Zhang, Yizhe  and
Yogatama, Dani  and
Ilharco, Gabriel  and
Pappas, Nikolaos  and
Mao, Yi  and
Chen, Weizhu  and
Smith, Noah A.},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2021.emnlp-main.830},
 editor = {Moens, Marie-Francine  and
Huang, Xuanjing  and
Specia, Lucia  and
Yih, Scott Wen-tau},
 pages = {10630--10643},
 publisher = {Association for Computational Linguistics},
 title = {Finetuning Pretrained Transformers into {RNN}s},
 url = {https://aclanthology.org/2021.emnlp-main.830},
 year = {2021}
}

@inproceedings{peng2021random,
 author = {Hao Peng and
Nikolaos Pappas and
Dani Yogatama and
Roy Schwartz and
Noah A. Smith and
Lingpeng Kong},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/Peng0Y0SK21.bib},
 booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
Virtual Event, Austria, May 3-7, 2021},
 publisher = {OpenReview.net},
 timestamp = {Wed, 23 Jun 2021 01:00:00 +0200},
 title = {Random Feature Attention},
 url = {https://openreview.net/forum?id=QtTKTdVrFBB},
 year = {2021}
}
@inproceedings{Poli2023HyenaHT,
  title={Hyena Hierarchy: Towards Larger Convolutional Language Models},
  author={Michael Poli and Stefano Massaroli and Eric Q. Nguyen and Daniel Y. Fu and Tri Dao and Stephen A. Baccus and Yoshua Bengio and Stefano Ermon and Christopher R{\'e}},
  booktitle={International Conference on Machine Learning},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:257050308}
}
@article{DBLP:journals/corr/abs-2403-19887,
  author       = {Opher Lieber and
                  Barak Lenz and
                  Hofit Bata and
                  Gal Cohen and
                  Jhonathan Osin and
                  Itay Dalmedigos and
                  Erez Safahi and
                  Shaked Meirom and
                  Yonatan Belinkov and
                  Shai Shalev{-}Shwartz and
                  Omri Abend and
                  Raz Alon and
                  Tomer Asida and
                  Amir Bergman and
                  Roman Glozman and
                  Michael Gokhman and
                  Avashalom Manevich and
                  Nir Ratner and
                  Noam Rozen and
                  Erez Shwartz and
                  Mor Zusman and
                  Yoav Shoham},
  title        = {Jamba: {A} Hybrid Transformer-Mamba Language Model},
  journal      = {CoRR},
  volume       = {abs/2403.19887},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2403.19887},
  doi          = {10.48550/ARXIV.2403.19887},
  eprinttype    = {arXiv},
  eprint       = {2403.19887},
  timestamp    = {Wed, 10 Apr 2024 17:37:45 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2403-19887.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{choromanski2020rethinking,
 author = {Krzysztof Marcin Choromanski and
Valerii Likhosherstov and
David Dohan and
Xingyou Song and
Andreea Gane and
Tam{\'{a}}s Sarl{\'{o}}s and
Peter Hawkins and
Jared Quincy Davis and
Afroz Mohiuddin and
Lukasz Kaiser and
David Benjamin Belanger and
Lucy J. Colwell and
Adrian Weller},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/ChoromanskiLDSG21.bib},
 booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
Virtual Event, Austria, May 3-7, 2021},
 publisher = {OpenReview.net},
 timestamp = {Wed, 23 Jun 2021 01:00:00 +0200},
 title = {Rethinking Attention with Performers},
 url = {https://openreview.net/forum?id=Ua6zuk0WRH},
 year = {2021}
}

@article{touvron2023llama,
 author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
 journal = {ArXiv preprint},
 title = {Llama: Open and efficient foundation language models},
 url = {https://arxiv.org/abs/2302.13971},
 volume = {abs/2302.13971},
 year = {2023}
}

@inproceedings{triton,
 author = {Philippe Tillet and
Hsiang{-}Tsung Kung and
David D. Cox},
 booktitle = {Proceedings of the 3rd {ACM} {SIGPLAN} International Workshop on Machine
Learning and Programming Languages, MAPL@PLDI 2019},
 doi = {10.1145/3315508.3329973},
 pages = {10--19},
 publisher = {{ACM}},
 title = {Triton: an intermediate language and compiler for tiled neural network
computations},
 year = {2019}
}

@inproceedings{rmsnorm,
 author = {Biao Zhang and
Rico Sennrich},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/ZhangS19a.bib},
 booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019, NeurIPS 2019, December
8-14, 2019, Vancouver, BC, Canada},
 editor = {Hanna M. Wallach and
Hugo Larochelle and
Alina Beygelzimer and
Florence d'Alch{\'{e}}{-}Buc and
Emily B. Fox and
Roman Garnett},
 pages = {12360--12371},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Root Mean Square Layer Normalization},
 url = {https://proceedings.neurips.cc/paper/2019/hash/1e8a19426224ca89e83cef47f1e7f53b-Abstract.html},
 year = {2019}
}

@inproceedings{paperno2016lambada,
 address = {Berlin, Germany},
 author = {Paperno, Denis  and
Kruszewski, Germ{\'a}n  and
Lazaridou, Angeliki  and
Pham, Ngoc Quan  and
Bernardi, Raffaella  and
Pezzelle, Sandro  and
Baroni, Marco  and
Boleda, Gemma  and
Fern{\'a}ndez, Raquel},
 booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/P16-1144},
 editor = {Erk, Katrin  and
Smith, Noah A.},
 pages = {1525--1534},
 publisher = {Association for Computational Linguistics},
 title = {The {LAMBADA} dataset: Word prediction requiring a broad discourse context},
 url = {https://aclanthology.org/P16-1144},
 year = {2016}
}

@inproceedings{zellers2019hellaswag,
 address = {Florence, Italy},
 author = {Zellers, Rowan  and
Holtzman, Ari  and
Bisk, Yonatan  and
Farhadi, Ali  and
Choi, Yejin},
 booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/P19-1472},
 editor = {Korhonen, Anna  and
Traum, David  and
M{\`a}rquez, Llu{\'\i}s},
 pages = {4791--4800},
 publisher = {Association for Computational Linguistics},
 title = {{H}ella{S}wag: Can a Machine Really Finish Your Sentence?},
 url = {https://aclanthology.org/P19-1472},
 year = {2019}
}

@inproceedings{bisk2020piqa,
 author = {Yonatan Bisk and
Rowan Zellers and
Ronan LeBras and
Jianfeng Gao and
Yejin Choi},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/aaai/BiskZLGC20.bib},
 booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
2020, The Thirty-Second Innovative Applications of Artificial Intelligence
Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
February 7-12, 2020},
 pages = {7432--7439},
 publisher = {{AAAI} Press},
 timestamp = {Thu, 04 Jun 2020 01:00:00 +0200},
 title = {{PIQA:} Reasoning about Physical Commonsense in Natural Language},
 url = {https://aaai.org/ojs/index.php/AAAI/article/view/6239},
 year = {2020}
}

@article{arc-ce,
 author = {Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
 journal = {ArXiv preprint},
 title = {Think you have solved question answering? try arc, the ai2 reasoning challenge},
 url = {https://arxiv.org/abs/1803.05457},
 volume = {abs/1803.05457},
 year = {2018}
}

@inproceedings{sakaguchi2021winogrande,
 author = {Keisuke Sakaguchi and
Ronan Le Bras and
Chandra Bhagavatula and
Yejin Choi},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/aaai/SakaguchiBBC20.bib},
 booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
2020, The Thirty-Second Innovative Applications of Artificial Intelligence
Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
February 7-12, 2020},
 pages = {8732--8740},
 publisher = {{AAAI} Press},
 timestamp = {Tue, 02 Feb 2021 00:00:00 +0100},
 title = {WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
 url = {https://aaai.org/ojs/index.php/AAAI/article/view/6399},
 year = {2020}
}

@article{reddy2019coqa,
 address = {Cambridge, MA},
 author = {Reddy, Siva  and
Chen, Danqi  and
Manning, Christopher D.},
 doi = {10.1162/tacl_a_00266},
 editor = {Lee, Lillian  and
Johnson, Mark  and
Roark, Brian  and
Nenkova, Ani},
 journal = {Transactions of the Association for Computational Linguistics},
 pages = {249--266},
 publisher = {MIT Press},
 title = {{C}o{QA}: A Conversational Question Answering Challenge},
 url = {https://aclanthology.org/Q19-1016},
 volume = {7},
 year = {2019}
}

@article{SciQA2023,
 author = {Auer, S{\"o}ren
and Barone, Dante A. C.
and Bartz, Cassiano
and Cortes, Eduardo G.
and Jaradeh, Mohamad Yaser
and Karras, Oliver
and Koubarakis, Manolis
and Mouromtsev, Dmitry
and Pliukhin, Dmitrii
and Radyush, Daniil
and Shilin, Ivan
and Stocker, Markus
and Tsalapati, Eleni},
 day = {04},
 doi = {10.1038/s41598-023-33607-z},
 issn = {2045-2322},
 journal = {Scientific Reports},
 number = {1},
 pages = {7240},
 title = {The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge},
 volume = {13},
 year = {2023}
}

@inproceedings{openbookqa,
 address = {Brussels, Belgium},
 author = {Mihaylov, Todor  and
Clark, Peter  and
Khot, Tushar  and
Sabharwal, Ashish},
 booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/D18-1260},
 editor = {Riloff, Ellen  and
Chiang, David  and
Hockenmaier, Julia  and
Tsujii, Jun{'}ichi},
 pages = {2381--2391},
 publisher = {Association for Computational Linguistics},
 title = {Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
 url = {https://aclanthology.org/D18-1260},
 year = {2018}
}

@inproceedings{clark2019boolq,
 address = {Minneapolis, Minnesota},
 author = {Clark, Christopher  and
Lee, Kenton  and
Chang, Ming-Wei  and
Kwiatkowski, Tom  and
Collins, Michael  and
Toutanova, Kristina},
 booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
 doi = {10.18653/v1/N19-1300},
 editor = {Burstein, Jill  and
Doran, Christy  and
Solorio, Thamar},
 pages = {2924--2936},
 publisher = {Association for Computational Linguistics},
 title = {{B}ool{Q}: Exploring the Surprising Difficulty of Natural Yes/No Questions},
 url = {https://aclanthology.org/N19-1300},
 year = {2019}
}

@inproceedings{xpos,
 address = {Toronto, Canada},
 author = {Sun, Yutao  and
Dong, Li  and
Patra, Barun  and
Ma, Shuming  and
Huang, Shaohan  and
Benhaim, Alon  and
Chaudhary, Vishrav  and
Song, Xia  and
Wei, Furu},
 booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2023.acl-long.816},
 editor = {Rogers, Anna  and
Boyd-Graber, Jordan  and
Okazaki, Naoaki},
 pages = {14590--14604},
 publisher = {Association for Computational Linguistics},
 title = {A Length-Extrapolatable Transformer},
 url = {https://aclanthology.org/2023.acl-long.816},
 year = {2023}
}

@software{eval-harness,
 author = {Gao, Leo and
Tow, Jonathan and
Biderman, Stella and
Black, Sid and
DiPofi, Anthony and
Foster, Charles and
Golding, Laurence and
Hsu, Jeffrey and
McDonell, Kyle and
Muennighoff, Niklas and
Phang, Jason and
Reynolds, Laria and
Tang, Eric and
Thite, Anish and
Wang, Ben and
Wang, Kevin and
Zou, Andy},
 doi = {10.5281/zenodo.5371628},
 publisher = {Zenodo},
 title = {A framework for few-shot language model evaluation},
 version = {v0.0.1},
 year = {2021}
}

@article{loshchilov2018fixing,
 author = {Loshchilov, Ilya and Hutter, Frank},
 title = {Fixing weight decay regularization in adam},
 year = {2018}
}

@misc{buckman2024,
 author = {Buckman, Jacob and Gelada, Carles},
 date = {2024-01-05},
 langid = {en},
 publisher = {Manifest AI},
 title = {Linear {Transformers} {Are} {Faster} {After} {All}},
 year = {2024}
}

@misc{cerebras2023slimpajama,
 author = {Soboleva, Daria and Al-Khateeb, Faisal and Myers, Robert and Steeves, Jacob R and Hestness, Joel and Dey, Nolan},
 title = {{SlimPajama: A 627B token cleaned and deduplicated version of RedPajama}},
 year = {2023}
}

@article{jiang2023mistral,
 author = {Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
 journal = {ArXiv preprint},
 title = {Mistral 7B},
 url = {https://arxiv.org/abs/2310.06825},
 volume = {abs/2310.06825},
 year = {2023}
}

@inproceedings{pg19,
 author = {Jack W. Rae and
Anna Potapenko and
Siddhant M. Jayakumar and
Chloe Hillier and
Timothy P. Lillicrap},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/RaePJHL20.bib},
 booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
Addis Ababa, Ethiopia, April 26-30, 2020},
 publisher = {OpenReview.net},
 timestamp = {Thu, 07 May 2020 01:00:00 +0200},
 title = {Compressive Transformers for Long-Range Sequence Modelling},
 url = {https://openreview.net/forum?id=SylKikSYDH},
 year = {2020}
}

@inproceedings{xiong2023effective,
 address = {Mexico City, Mexico},
 author = {Xiong, Wenhan  and
Liu, Jingyu  and
Molybog, Igor  and
Zhang, Hejia  and
Bhargava, Prajjwal  and
Hou, Rui  and
Martin, Louis  and
Rungta, Rashi  and
Sankararaman, Karthik Abinav  and
Oguz, Barlas  and
Khabsa, Madian  and
Fang, Han  and
Mehdad, Yashar  and
Narang, Sharan  and
Malik, Kshitiz  and
Fan, Angela  and
Bhosale, Shruti  and
Edunov, Sergey  and
Lewis, Mike  and
Wang, Sinong  and
Ma, Hao},
 booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
 editor = {Duh, Kevin  and
Gomez, Helena  and
Bethard, Steven},
 pages = {4643--4663},
 publisher = {Association for Computational Linguistics},
 title = {Effective Long-Context Scaling of Foundation Models},
 url = {https://aclanthology.org/2024.naacl-long.260},
 year = {2024}
}

@article{chen2023extending,
 author = {Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
 journal = {ArXiv preprint},
 title = {Extending context window of large language models via positional interpolation},
 url = {https://arxiv.org/abs/2306.15595},
 volume = {abs/2306.15595},
 year = {2023}
}

@article{fire2024,
 author = {Li, Shanda and You, Chong and Guruganesh, Guru and Ainslie, Joshua and Ontanon, Santiago and Zaheer, Manzil and Sanghai, Sumit and Yang, Yiming and Kumar, Sanjiv and Bhojanapalli, Srinadh},
 journal = {ArXiv preprint},
 title = {Functional interpolation for relative positions improves long context transformers},
 url = {https://arxiv.org/abs/2310.04418},
 volume = {abs/2310.04418},
 year = {2023}
}

@article{Sun2024LinearAS,
 author = {Sun, Weigao and Qin, Zhen and Li, Dong and Shen, Xuyang and Qiao, Yu and Zhong, Yiran},
 journal = {ArXiv preprint},
 title = {Linear Attention Sequence Parallelism},
 url = {https://arxiv.org/abs/2404.02882},
 volume = {abs/2404.02882},
 year = {2024}
}

@inproceedings{mathiasen2020faster,
 author = {Mathiasen, Alexander and Hvilsh{\o}j, Frederik and J{\o}rgensen, Jakob R{\o}dsgaard and Nasery, Anshul and Mottin, Davide},
 booktitle = {ICML, Workshop Proceedings},
 title = {Faster Orthogonal Parameterization with Householder Matrices},
 year = {2020}
}

@article{Lieber2024JambaAH,
 author = {Lieber, Opher and Lenz, Barak and Bata, Hofit and Cohen, Gal and Osin, Jhonathan and Dalmedigos, Itay and Safahi, Erez and Meirom, Shaked and Belinkov, Yonatan and Shalev-Shwartz, Shai and others},
 journal = {ArXiv preprint},
 title = {Jamba: A hybrid transformer-mamba language model},
 url = {https://arxiv.org/abs/2403.19887},
 volume = {abs/2403.19887},
 year = {2024}
}

@inproceedings{Ma2022MegaMA,
 author = {Xuezhe Ma and
Chunting Zhou and
Xiang Kong and
Junxian He and
Liangke Gui and
Graham Neubig and
Jonathan May and
Luke Zettlemoyer},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/MaZKHGNMZ23.bib},
 booktitle = {The Eleventh International Conference on Learning Representations,
{ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
 publisher = {OpenReview.net},
 timestamp = {Fri, 30 Jun 2023 01:00:00 +0200},
 title = {Mega: Moving Average Equipped Gated Attention},
 url = {https://openreview.net/pdf?id=qNLe3iq2El},
 year = {2023}
}

@article{Mercat2024LinearizingLL,
 author = {Mercat, Jean and Vasiljevic, Igor and Keh, Sedrick and Arora, Kushal and Dave, Achal and Gaidon, Adrien and Kollar, Thomas},
 journal = {ArXiv preprint},
 title = {Linearizing Large Language Models},
 url = {https://arxiv.org/abs/2405.06640},
 volume = {abs/2405.06640},
 year = {2024}
}

@article{Sun2024YouOC,
 author = {Sun, Yutao and Dong, Li and Zhu, Yi and Huang, Shaohan and Wang, Wenhui and Ma, Shuming and Zhang, Quanlu and Wang, Jianyong and Wei, Furu},
 journal = {ArXiv preprint},
 title = {You Only Cache Once: Decoder-Decoder Architectures for Language Models},
 url = {https://arxiv.org/abs/2405.05254},
 volume = {abs/2405.05254},
 year = {2024}
}

@article{Lei2022SimpleRI,
 author = {Lei, Tao and Tian, Ran and Bastings, Jasmijn and Parikh, Ankur P},
 journal = {ArXiv preprint},
 title = {Simple Recurrence Improves Masked Language Models},
 url = {https://arxiv.org/abs/2205.11588},
 volume = {abs/2205.11588},
 year = {2022}
}

@article{Park2024CanML,
 author = {Park, Jongho and Park, Jaeseung and Xiong, Zheyang and Lee, Nayoung and Cho, Jaewoong and Oymak, Samet and Lee, Kangwook and Papailiopoulos, Dimitris},
 journal = {ArXiv preprint},
 title = {Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks},
 url = {https://arxiv.org/abs/2402.04248},
 volume = {abs/2402.04248},
 year = {2024}
}

@article{Ma2024MegalodonEL,
 author = {Ma, Xuezhe and Yang, Xiaomeng and Xiong, Wenhan and Chen, Beidi and Yu, Lili and Zhang, Hao and May, Jonathan and Zettlemoyer, Luke and Levy, Omer and Zhou, Chunting},
 journal = {ArXiv preprint},
 title = {Megalodon: Efficient llm pretraining and inference with unlimited context length},
 url = {https://arxiv.org/abs/2404.08801},
 volume = {abs/2404.08801},
 year = {2024}
}

@article{Fathi2023BlockStateT,
 author = {Fathi, Mahan and Pilault, Jonathan and Bacon, Pierre-Luc and Pal, Christopher and Firat, Orhan and Goroshin, Ross},
 journal = {ArXiv preprint},
 title = {Block-state transformer},
 url = {https://arxiv.org/abs/2306.09539},
 volume = {abs/2306.09539},
 year = {2023}
}

@inproceedings{zhang-etal-2023-efficient-long,
 address = {Singapore},
 author = {Zhang, Qingru  and
Ram, Dhananjay  and
Hawkins, Cole  and
Zha, Sheng  and
Zhao, Tuo},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
 doi = {10.18653/v1/2023.findings-emnlp.183},
 editor = {Bouamor, Houda  and
Pino, Juan  and
Bali, Kalika},
 pages = {2775--2786},
 publisher = {Association for Computational Linguistics},
 title = {Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer},
 url = {https://aclanthology.org/2023.findings-emnlp.183},
 year = {2023}
}

@article{Lingashetty2010DeltaLR,
 author = {Lingashetty, Krishna Chaithanya},
 journal = {arXiv preprint arXiv:1007.0417},
 title = {Delta Learning Rule for the Active Sites Model},
 year = {2010}
}

@article{Prados1989NeuralNC,
 author = {Prados, DL and Kak, SC},
 journal = {Electronics Letters},
 number = {25},
 pages = {197--199},
 title = {Neural network capacity using delta rule},
 volume = {3},
 year = {1989}
}

@article{McEliece1987TheCO,
 author = {Robert J. McEliece and Edward C. Posner and Eugene R. Rodemich and Santosh S. Venkatesh},
 journal = {IEEE Trans. Inf. Theory},
 pages = {461-482},
 title = {The capacity of the Hopfield associative memory},
 volume = {33},
 year = {1987}
}

@inproceedings{alibi2021,
 author = {Ofir Press and
Noah A. Smith and
Mike Lewis},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/PressSL22.bib},
 booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
 publisher = {OpenReview.net},
 timestamp = {Tue, 27 Dec 2022 00:00:00 +0100},
 title = {Train Short, Test Long: Attention with Linear Biases Enables Input
Length Extrapolation},
 url = {https://openreview.net/forum?id=R8sQPpGCv0},
 year = {2022}
}

@inproceedings{xpos2022,
 address = {Toronto, Canada},
 author = {Sun, Yutao  and
Dong, Li  and
Patra, Barun  and
Ma, Shuming  and
Huang, Shaohan  and
Benhaim, Alon  and
Chaudhary, Vishrav  and
Song, Xia  and
Wei, Furu},
 booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2023.acl-long.816},
 editor = {Rogers, Anna  and
Boyd-Graber, Jordan  and
Okazaki, Naoaki},
 pages = {14590--14604},
 publisher = {Association for Computational Linguistics},
 title = {A Length-Extrapolatable Transformer},
 url = {https://aclanthology.org/2023.acl-long.816},
 year = {2023}
}

@inproceedings{gupta2022diagonal,
 author = {Ankit Gupta and
Albert Gu and
Jonathan Berant},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/0001GB22.bib},
 booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022},
 editor = {Sanmi Koyejo and
S. Mohamed and
A. Agarwal and
Danielle Belgrave and
K. Cho and
A. Oh},
 timestamp = {Mon, 08 Jan 2024 00:00:00 +0100},
 title = {Diagonal State Spaces are as Effective as Structured State Spaces},
 url = {http://papers.nips.cc/paper\_files/paper/2022/hash/9156b0f6dfa9bbd18c79cc459ef5d61c-Abstract-Conference.html},
 year = {2022}
}

@inproceedings{smith2022simplified,
 author = {Jimmy T. H. Smith and
Andrew Warrington and
Scott W. Linderman},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/SmithWL23.bib},
 booktitle = {The Eleventh International Conference on Learning Representations,
{ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
 publisher = {OpenReview.net},
 timestamp = {Fri, 30 Jun 2023 01:00:00 +0200},
 title = {Simplified State Space Layers for Sequence Modeling},
 url = {https://openreview.net/pdf?id=Ai8Hw3AXqks},
 year = {2023}
}

@inproceedings{ba_using_2016,
 author = {Jimmy Ba and
Geoffrey E. Hinton and
Volodymyr Mnih and
Joel Z. Leibo and
Catalin Ionescu},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/BaHMLI16.bib},
 booktitle = {Advances in Neural Information Processing Systems 29: Annual Conference
on Neural Information Processing Systems 2016, December 5-10, 2016,
Barcelona, Spain},
 editor = {Daniel D. Lee and
Masashi Sugiyama and
Ulrike von Luxburg and
Isabelle Guyon and
Roman Garnett},
 pages = {4331--4339},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Using Fast Weights to Attend to the Recent Past},
 url = {https://proceedings.neurips.cc/paper/2016/hash/9f44e956e3a2b7b5598c625fcc802c36-Abstract.html},
 year = {2016}
}

@article{schmidhuber_learning_1992,
 author = {Schmidhuber, Jürgen},
 journal = {Neural Computation},
 number = {1},
 pages = {131--139},
 title = {Learning to control fast-weight memories: {An} alternative to dynamic recurrent networks},
 volume = {4},
 year = {1992}
}

@inproceedings{irie_going_2021,
 author = {Kazuki Irie and
Imanol Schlag and
R{\'{o}}bert Csord{\'{a}}s and
J{\"{u}}rgen Schmidhuber},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/IrieSCS21.bib},
 booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
on Neural Information Processing Systems 2021, NeurIPS 2021, December
6-14, 2021, virtual},
 editor = {Marc'Aurelio Ranzato and
Alina Beygelzimer and
Yann N. Dauphin and
Percy Liang and
Jennifer Wortman Vaughan},
 pages = {7703--7717},
 timestamp = {Tue, 03 May 2022 01:00:00 +0200},
 title = {Going Beyond Linear Transformers with Recurrent Fast Weight Programmers},
 url = {https://proceedings.neurips.cc/paper/2021/hash/3f9e3767ef3b10a0de4c256d7ef9805d-Abstract.html},
 year = {2021}
}

@inproceedings{martin_parallelizing_2018,
 author = {Eric Martin and
Chris Cundy},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/MartinC18.bib},
 booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
 publisher = {OpenReview.net},
 timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
 title = {Parallelizing Linear Recurrent Neural Nets Over Sequence Length},
 url = {https://openreview.net/forum?id=HyUNwulC-},
 year = {2018}
}

@inproceedings{hinton_using_1987,
 author = {Hinton, Geoffrey E and Plaut, David C},
 booktitle = {Proceedings of the ninth annual conference of the {Cognitive} {Science} {Society}},
 pages = {177--186},
 title = {Using fast weights to deblur old memories},
 year = {1987}
}

@inproceedings{schlag_linear_2021,
 author = {Imanol Schlag and
Kazuki Irie and
J{\"{u}}rgen Schmidhuber},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/SchlagIS21.bib},
 booktitle = {Proceedings of the 38th International Conference on Machine Learning,
{ICML} 2021, 18-24 July 2021, Virtual Event},
 editor = {Marina Meila and
Tong Zhang},
 pages = {9355--9366},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Wed, 25 Aug 2021 01:00:00 +0200},
 title = {Linear Transformers Are Secretly Fast Weight Programmers},
 url = {http://proceedings.mlr.press/v139/schlag21a.html},
 volume = {139},
 year = {2021}
}

@article{westhuizen_unreasonable_2018,
 author = {Westhuizen, Jos van der and Lasenby, Joan},
 journal = {ArXiv preprint},
 title = {The unreasonable effectiveness of the forget gate},
 url = {https://arxiv.org/abs/1804.04849},
 volume = {abs/1804.04849},
 year = {2018}
}

@inproceedings{mao_fine-tuning_2022,
 address = {Abu Dhabi, United Arab Emirates},
 author = {Mao, Huanru Henry},
 booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2022.emnlp-main.697},
 editor = {Goldberg, Yoav  and
Kozareva, Zornitsa  and
Zhang, Yue},
 pages = {10236--10242},
 publisher = {Association for Computational Linguistics},
 title = {Fine-Tuning Pre-trained Transformers into Decaying Fast Weights},
 url = {https://aclanthology.org/2022.emnlp-main.697},
 year = {2022}
}

@inproceedings{hua_transformer_2022,
 author = {Weizhe Hua and
Zihang Dai and
Hanxiao Liu and
Quoc V. Le},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/HuaDLL22.bib},
 booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
2022, Baltimore, Maryland, {USA}},
 editor = {Kamalika Chaudhuri and
Stefanie Jegelka and
Le Song and
Csaba Szepesv{\'{a}}ri and
Gang Niu and
Sivan Sabato},
 pages = {9099--9117},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Tue, 12 Jul 2022 01:00:00 +0200},
 title = {Transformer Quality in Linear Time},
 url = {https://proceedings.mlr.press/v162/hua22a.html},
 volume = {162},
 year = {2022}
}

@article{lingle_transformer-vq_2023,
 author = {Lingle, Lucas D},
 journal = {ArXiv preprint},
 title = {Transformer-vq: Linear-time transformers via vector quantization},
 url = {https://arxiv.org/abs/2309.16354},
 volume = {abs/2309.16354},
 year = {2023}
}

@inproceedings{qin_hierarchically_2023,
 author = {Zhen Qin and
Songlin Yang and
Yiran Zhong},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/QinYZ23.bib},
 booktitle = {Advances in Neural Information Processing Systems 36: Annual Conference
on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023},
 editor = {Alice Oh and
Tristan Naumann and
Amir Globerson and
Kate Saenko and
Moritz Hardt and
Sergey Levine},
 timestamp = {Fri, 01 Mar 2024 00:00:00 +0100},
 title = {Hierarchically Gated Recurrent Neural Network for Sequence Modeling},
 url = {http://papers.nips.cc/paper\_files/paper/2023/hash/694be3548697e9cc8999d45e8d16fe1e-Abstract-Conference.html},
 year = {2023}
}

@inproceedings{peng_rwkv_2023,
 address = {Singapore},
 author = {Peng, Bo  and
Alcaide, Eric  and
Anthony, Quentin  and
Albalak, Alon  and
Arcadinho, Samuel  and
Biderman, Stella  and
Cao, Huanqi  and
Cheng, Xin  and
Chung, Michael  and
Derczynski, Leon  and
Du, Xingjian  and
Grella, Matteo  and
Gv, Kranthi  and
He, Xuzheng  and
Hou, Haowen  and
Kazienko, Przemyslaw  and
Kocon, Jan  and
Kong, Jiaming  and
Koptyra, Bart{\l}omiej  and
Lau, Hayden  and
Lin, Jiaju  and
Mantri, Krishna Sri Ipsit  and
Mom, Ferdinand  and
Saito, Atsushi  and
Song, Guangyu  and
Tang, Xiangru  and
Wind, Johan  and
Wo{\'z}niak, Stanis{\l}aw  and
Zhang, Zhenyuan  and
Zhou, Qinghua  and
Zhu, Jian  and
Zhu, Rui-Jie},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
 doi = {10.18653/v1/2023.findings-emnlp.936},
 editor = {Bouamor, Houda  and
Pino, Juan  and
Bali, Kalika},
 pages = {14048--14077},
 publisher = {Association for Computational Linguistics},
 title = {{RWKV}: Reinventing {RNN}s for the Transformer Era},
 url = {https://aclanthology.org/2023.findings-emnlp.936},
 year = {2023}
}

@inproceedings{gu_efficiently_2022,
 author = {Albert Gu and
Karan Goel and
Christopher R{\'{e}}},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/GuGR22.bib},
 booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
 publisher = {OpenReview.net},
 timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
 title = {Efficiently Modeling Long Sequences with Structured State Spaces},
 url = {https://openreview.net/forum?id=uYLFoz1vlAC},
 year = {2022}
}

@inproceedings{katharopoulos_transformers_2020,
 author = {Angelos Katharopoulos and
Apoorv Vyas and
Nikolaos Pappas and
Fran{\c{c}}ois Fleuret},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/KatharopoulosV020.bib},
 booktitle = {Proceedings of the 37th International Conference on Machine Learning,
{ICML} 2020, 13-18 July 2020, Virtual Event},
 pages = {5156--5165},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Tue, 15 Dec 2020 00:00:00 +0100},
 title = {Transformers are RNNs: Fast Autoregressive Transformers with Linear
Attention},
 url = {http://proceedings.mlr.press/v119/katharopoulos20a.html},
 volume = {119},
 year = {2020}
}

@article{dao_flashattention-2_2023,
 author = {Dao, Tri},
 journal = {ArXiv preprint},
 title = {{FlashAttention}-2: {Faster} {Attention} with {Better} {Parallelism} and {Work} {Partitioning}},
 url = {https://arxiv.org/abs/2307.08691},
 volume = {abs/2307.08691},
 year = {2023}
}

@article{sun_retentive_2023,
 author = {Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu},
 journal = {ArXiv preprint},
 title = {Retentive network: {A} successor to transformer for large language models},
 url = {https://arxiv.org/abs/2307.08621},
 volume = {abs/2307.08621},
 year = {2023}
}

@inproceedings{dao_flashattention_2022,
 author = {Tri Dao and
Daniel Y. Fu and
Stefano Ermon and
Atri Rudra and
Christopher R{\'{e}}},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/DaoFERR22.bib},
 booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022},
 editor = {Sanmi Koyejo and
S. Mohamed and
A. Agarwal and
Danielle Belgrave and
K. Cho and
A. Oh},
 timestamp = {Mon, 08 Jan 2024 00:00:00 +0100},
 title = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
 url = {http://papers.nips.cc/paper\_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html},
 year = {2022}
}

@inproceedings{kasai_finetuning_2021,
 address = {Online and Punta Cana, Dominican Republic},
 author = {Kasai, Jungo  and
Peng, Hao  and
Zhang, Yizhe  and
Yogatama, Dani  and
Ilharco, Gabriel  and
Pappas, Nikolaos  and
Mao, Yi  and
Chen, Weizhu  and
Smith, Noah A.},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2021.emnlp-main.830},
 editor = {Moens, Marie-Francine  and
Huang, Xuanjing  and
Specia, Lucia  and
Yih, Scott Wen-tau},
 pages = {10630--10643},
 publisher = {Association for Computational Linguistics},
 title = {Finetuning Pretrained Transformers into {RNN}s},
 url = {https://aclanthology.org/2021.emnlp-main.830},
 year = {2021}
}

@article{su_roformer_2021,
 author = {Su, Jianlin and Lu, Yu and Pan, Shengfeng and Wen, Bo and Liu, Yunfeng},
 journal = {ArXiv preprint},
 title = {{RoFormer}: {Enhanced} {Transformer} with {Rotary} {Position} {Embedding}},
 url = {https://arxiv.org/abs/2104.09864},
 volume = {abs/2104.09864},
 year = {2021}
}

@article{ramachandran_swish_2017,
 author = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
 journal = {arXiv: Neural and Evolutionary Computing},
 title = {Swish: a {Self}-{Gated} {Activation} {Function}},
 year = {2017}
}

@inproceedings{pisha_accelerating_2021,
 author = {Pisha, Louis and Ligowski, Lukasz},
 booktitle = {35th {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium}, {IPDPS} 2021, {Portland}, {OR}, {USA}, {May} 17-21, 2021},
 pages = {507--516},
 publisher = {IEEE},
 title = {Accelerating non-power-of-2 size {Fourier} transforms with {GPU} {Tensor} {Cores}},
 year = {2021}
}

@article{li_tcfft_2021,
 author = {Li, Bin-Rui and Cheng, Shenggan and Lin, James},
 journal = {ArXiv preprint},
 title = {{tcFFT}: {Accelerating} {Half}-{Precision} {FFT} through {Tensor} {Cores}},
 url = {https://arxiv.org/abs/2104.11471},
 volume = {abs/2104.11471},
 year = {2021}
}

@inproceedings{cho_learning_2014,
 address = {Doha, Qatar},
 author = {Cho, Kyunghyun  and
van Merri{\"e}nboer, Bart  and
Gulcehre, Caglar  and
Bahdanau, Dzmitry  and
Bougares, Fethi  and
Schwenk, Holger  and
Bengio, Yoshua},
 booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
 doi = {10.3115/v1/D14-1179},
 editor = {Moschitti, Alessandro  and
Pang, Bo  and
Daelemans, Walter},
 pages = {1724--1734},
 publisher = {Association for Computational Linguistics},
 title = {Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation},
 url = {https://aclanthology.org/D14-1179},
 year = {2014}
}

@article{hochreiter_long_1997,
 author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
 journal = {Neural Computation},
 number = {8},
 pages = {1735--1780},
 title = {Long {Short}-{Term} {Memory}},
 volume = {9},
 year = {1997}
}

@article{shazeer_glu_2020,
 author = {Shazeer, Noam},
 journal = {ArXiv preprint},
 title = {Glu variants improve transformer},
 url = {https://arxiv.org/abs/2002.05202},
 volume = {abs/2002.05202},
 year = {2020}
}

@inproceedings{mehta_long_2023,
 author = {Harsh Mehta and
Ankit Gupta and
Ashok Cutkosky and
Behnam Neyshabur},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/Mehta0CN23.bib},
 booktitle = {The Eleventh International Conference on Learning Representations,
{ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
 publisher = {OpenReview.net},
 timestamp = {Fri, 30 Jun 2023 01:00:00 +0200},
 title = {Long Range Language Modeling via Gated State Spaces},
 url = {https://openreview.net/pdf?id=5MkYIYCbva},
 year = {2023}
}

@inproceedings{wang_pretraining_2022,
 address = {Singapore},
 author = {Wang, Junxiong  and
Yan, Jing Nathan  and
Gu, Albert  and
Rush, Alexander},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
 doi = {10.18653/v1/2023.findings-emnlp.5},
 editor = {Bouamor, Houda  and
Pino, Juan  and
Bali, Kalika},
 pages = {58--69},
 publisher = {Association for Computational Linguistics},
 title = {Pretraining Without Attention},
 url = {https://aclanthology.org/2023.findings-emnlp.5},
 year = {2023}
}

@inproceedings{dakkak_accelerating_2019,
 author = {Dakkak, Abdul and Li, Cheng and Xiong, Jinjun and Gelado, Isaac and Hwu, Wen-Mei W.},
 booktitle = {Proceedings of the {ACM} {International} {Conference} on {Supercomputing}, {ICS} 2019, {Phoenix}, {AZ}, {USA}, {June} 26-28, 2019},
 editor = {Eigenmann, Rudolf and Ding, Chen and McKee, Sally A.},
 pages = {46--57},
 publisher = {ACM},
 title = {Accelerating reduction and scan using tensor core units},
 year = {2019}
}

@article{fu_flashfftconv_2023,
 author = {Fu, Daniel Y. and Kumbong, Hermann and Nguyen, Eric and Ré, Christopher},
 journal = {ArXiv preprint},
 title = {{FlashFFTConv}: {Efficient} {Convolutions} for {Long} {Sequences} with {Tensor} {Cores}},
 url = {https://arxiv.org/abs/2311.05908},
 volume = {abs/2311.05908},
 year = {2023}
}

@inproceedings{choquette_32_2021,
 author = {Choquette, Jack and Lee, Edward and Krashinsky, Ronny and Balan, Vishnu and Khailany, Brucek},
 booktitle = {2021 {IEEE} {International} {Solid}-{State} {Circuits} {Conference} ({ISSCC})},
 doi = {10.1109/ISSCC42613.2021.9365803},
 pages = {48--50},
 title = {3.2 {The} {A100} {Datacenter} {GPU} and {Ampere} {Architecture}},
 volume = {64},
 year = {2021}
}

@inproceedings{gu_mamba_2023,
 author = {Gu, Albert and Dao, Tri},
 booktitle = {Proceedings of COLM},
 title = {Mamba: {Linear}-{Time} {Sequence} {Modeling} with {Selective} {State} {Spaces}},
 year = {2023}
}

@inproceedings{li_sequence_2023,
 address = {Toronto, Canada},
 author = {Li, Shenggui  and
Xue, Fuzhao  and
Baranwal, Chaitanya  and
Li, Yongbin  and
You, Yang},
 booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2023.acl-long.134},
 editor = {Rogers, Anna  and
Boyd-Graber, Jordan  and
Okazaki, Naoaki},
 pages = {2391--2404},
 publisher = {Association for Computational Linguistics},
 title = {Sequence Parallelism: Long Sequence Training from System Perspective},
 url = {https://aclanthology.org/2023.acl-long.134},
 year = {2023}
}

@inproceedings{dao_monarch_2022,
 author = {Tri Dao and
Beidi Chen and
Nimit Sharad Sohoni and
Arjun D. Desai and
Michael Poli and
Jessica Grogan and
Alexander Liu and
Aniruddh Rao and
Atri Rudra and
Christopher R{\'{e}}},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/DaoCSDPGLRRR22.bib},
 booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
2022, Baltimore, Maryland, {USA}},
 editor = {Kamalika Chaudhuri and
Stefanie Jegelka and
Le Song and
Csaba Szepesv{\'{a}}ri and
Gang Niu and
Sivan Sabato},
 pages = {4690--4721},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Tue, 12 Jul 2022 01:00:00 +0200},
 title = {Monarch: Expressive Structured Matrices for Efficient and Accurate
Training},
 url = {https://proceedings.mlr.press/v162/dao22a.html},
 volume = {162},
 year = {2022}
}

@inproceedings{chaurasia_compiling_2015,
 author = {Chaurasia, Gaurav and Ragan-Kelley, Jonathan and Paris, Sylvain and Drettakis, George and Durand, Frédo},
 booktitle = {High {Performance} {Graphics}},
 title = {Compiling high performance recursive filters},
 year = {2015}
}

@inproceedings{yan_diffusion_2023,
 author = {Yan, Jing Nathan and Gu, Jiatao and Rush, Alexander M.},
 title = {Diffusion {Models} {Without} {Attention}},
 year = {2023}
}

@article{hooker_hardware_2020,
 author = {Hooker, Sara},
 journal = {Communications of the ACM},
 pages = {58 -- 65},
 title = {The hardware lottery},
 volume = {64},
 year = {2020}
}

@inproceedings{fu_monarch_2023,
 author = {Daniel Y. Fu and
Simran Arora and
Jessica Grogan and
Isys Johnson and
Evan Sabri Eyuboglu and
Armin W. Thomas and
Benjamin Spector and
Michael Poli and
Atri Rudra and
Christopher R{\'{e}}},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/FuAGJETSPRR23.bib},
 booktitle = {Advances in Neural Information Processing Systems 36: Annual Conference
on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023},
 editor = {Alice Oh and
Tristan Naumann and
Amir Globerson and
Kate Saenko and
Moritz Hardt and
Sergey Levine},
 timestamp = {Fri, 01 Mar 2024 00:00:00 +0100},
 title = {Monarch Mixer: {A} Simple Sub-Quadratic GEMM-Based Architecture},
 url = {http://papers.nips.cc/paper\_files/paper/2023/hash/f498c1ce6bff52eb04febf87438dd84b-Abstract-Conference.html},
 year = {2023}
}

@article{liu_ring_2023,
 author = {Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
 journal = {ArXiv preprint},
 title = {Ring {Attention} with {Blockwise} {Transformers} for {Near}-{Infinite} {Context}},
 url = {https://arxiv.org/abs/2310.01889},
 volume = {abs/2310.01889},
 year = {2023}
}

@article{brandon_striped_2023,
 author = {Brandon, William and Nrusimha, Aniruddha and Qian, Kevin and Ankner, Zachary and Jin, Tian and Song, Zhiye and Ragan-Kelley, Jonathan},
 journal = {ArXiv preprint},
 title = {Striped {Attention}: {Faster} {Ring} {Attention} for {Causal} {Transformers}},
 url = {https://arxiv.org/abs/2311.09431},
 volume = {abs/2311.09431},
 year = {2023}
}

@inproceedings{saphra_first_2023,
 address = {Mexico City, Mexico},
 author = {Saphra, Naomi  and
Fleisig, Eve  and
Cho, Kyunghyun  and
Lopez, Adam},
 booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
 editor = {Duh, Kevin  and
Gomez, Helena  and
Bethard, Steven},
 pages = {2310--2326},
 publisher = {Association for Computational Linguistics},
 title = {First Tragedy, then Parse: History Repeats Itself in the New Era of Large Language Models},
 url = {https://aclanthology.org/2024.naacl-long.128},
 year = {2024}
}

@article{pramanik_recurrent_2023,
 author = {Pramanik, Subhojeet and Elelimy, Esraa and Machado, Marlos C. and White, Adam},
 journal = {ArXiv preprint},
 title = {Recurrent {Linear} {Transformers}},
 url = {https://arxiv.org/abs/2310.15719},
 volume = {abs/2310.15719},
 year = {2023}
}

@inproceedings{blelloch_prefix_1990,
 author = {Blelloch, Guy E.},
 title = {Prefix sums and their applications},
 year = {1990}
}

@article{li_lightseq_2023,
 author = {Li, Dacheng and Shao, Rulin and Xie, Anze and Xing, Eric P. and Gonzalez, Joseph E. and Stoica, Ion and Ma, Xuezhe and Zhang, Hao},
 journal = {ArXiv preprint},
 title = {{LightSeq}: {Sequence} {Level} {Parallelism} for {Distributed} {Training} of {Long} {Context} {Transformers}},
 url = {https://arxiv.org/abs/2310.03294},
 volume = {abs/2310.03294},
 year = {2023}
}

@article{gers_learning_2000,
 author = {Gers, Felix A. and Schmidhuber, Jürgen and Cummins, Fred A.},
 journal = {Neural Comput.},
 number = {10},
 pages = {2451--2471},
 title = {Learning to {Forget}: {Continual} {Prediction} with {LSTM}},
 volume = {12},
 year = {2000}
}

@inproceedings{smith_simplified_2023,
 author = {Jimmy T. H. Smith and
Andrew Warrington and
Scott W. Linderman},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/SmithWL23.bib},
 booktitle = {The Eleventh International Conference on Learning Representations,
{ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
 publisher = {OpenReview.net},
 timestamp = {Fri, 30 Jun 2023 01:00:00 +0200},
 title = {Simplified State Space Layers for Sequence Modeling},
 url = {https://openreview.net/pdf?id=Ai8Hw3AXqks},
 year = {2023}
}

@article{arora_zoology_2023,
 author = {Arora, Simran and Eyuboglu, Sabri and Timalsina, Aman and Johnson, Isys and Poli, Michael and Zou, James and Rudra, Atri and Ré, Christopher},
 journal = {ArXiv preprint},
 title = {Zoology: {Measuring} and {Improving} {Recall} in {Efficient} {Language} {Models}},
 url = {https://arxiv.org/abs/2312.04927},
 volume = {abs/2312.04927},
 year = {2023}
}

@inproceedings{fu_hungry_2023,
 author = {Daniel Y. Fu and
Tri Dao and
Khaled Kamal Saab and
Armin W. Thomas and
Atri Rudra and
Christopher R{\'{e}}},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/FuDSTRR23.bib},
 booktitle = {The Eleventh International Conference on Learning Representations,
{ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
 publisher = {OpenReview.net},
 timestamp = {Fri, 30 Jun 2023 01:00:00 +0200},
 title = {Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
 url = {https://openreview.net/pdf?id=COZDy0WYGg},
 year = {2023}
}

@article{qin_lightning_2024,
 author = {Qin, Zhen and Sun, Weigao and Li, Dong and Shen, Xuyang and Sun, Weixuan and Zhong, Yiran},
 journal = {ArXiv preprint},
 title = {Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models},
 url = {https://arxiv.org/abs/2401.04658},
 volume = {abs/2401.04658},
 year = {2024}
}

@inproceedings{peng_abc_2022,
 address = {Dublin, Ireland},
 author = {Peng, Hao  and
Kasai, Jungo  and
Pappas, Nikolaos  and
Yogatama, Dani  and
Wu, Zhaofeng  and
Kong, Lingpeng  and
Schwartz, Roy  and
Smith, Noah A.},
 booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2022.acl-long.515},
 editor = {Muresan, Smaranda  and
Nakov, Preslav  and
Villavicencio, Aline},
 pages = {7469--7483},
 publisher = {Association for Computational Linguistics},
 title = {{ABC}: Attention with Bounded-memory Control},
 url = {https://aclanthology.org/2022.acl-long.515},
 year = {2022}
}

@misc{zhang_linear_2023,
 author = {Zhang, Jun and Jiang, Shuyang and Feng, Jiangtao and Zheng, Lin and Kong, Lingpeng},
 journal = {ArXiv preprint},
 title = {Linear {Attention} via {Orthogonal} {Memory}},
 url = {https://arxiv.org/abs/2312.11135},
 volume = {abs/2312.11135},
 year = {2023}
}

@inproceedings{alman_fast_2023,
 author = {Josh Alman and
Zhao Song},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/Alman023.bib},
 booktitle = {Advances in Neural Information Processing Systems 36: Annual Conference
on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023},
 editor = {Alice Oh and
Tristan Naumann and
Amir Globerson and
Kate Saenko and
Moritz Hardt and
Sergey Levine},
 timestamp = {Fri, 01 Mar 2024 00:00:00 +0100},
 title = {Fast Attention Requires Bounded Entries},
 url = {http://papers.nips.cc/paper\_files/paper/2023/hash/c72861451d6fa9dfa64831102b9bb71a-Abstract-Conference.html},
 year = {2023}
}

@inproceedings{poli_hyena_2023,
 author = {Michael Poli and
Stefano Massaroli and
Eric Nguyen and
Daniel Y. Fu and
Tri Dao and
Stephen Baccus and
Yoshua Bengio and
Stefano Ermon and
Christopher R{\'{e}}},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/PoliMNFDBBER23.bib},
 booktitle = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
2023, Honolulu, Hawaii, {USA}},
 editor = {Andreas Krause and
Emma Brunskill and
Kyunghyun Cho and
Barbara Engelhardt and
Sivan Sabato and
Jonathan Scarlett},
 pages = {28043--28078},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Mon, 28 Aug 2023 01:00:00 +0200},
 title = {Hyena Hierarchy: Towards Larger Convolutional Language Models},
 url = {https://proceedings.mlr.press/v202/poli23a.html},
 volume = {202},
 year = {2023}
}

@misc{kacham_polysketchformer_2023,
 author = {Kacham, Praneeth and Mirrokni, Vahab and Zhong, Peilin},
 journal = {ArXiv preprint},
 title = {{PolySketchFormer}: {Fast} {Transformers} via {Sketching} {Polynomial} {Kernels}},
 url = {https://arxiv.org/abs/2310.01655},
 volume = {abs/2310.01655},
 year = {2023}
}

@inproceedings{zhang_linearizing_2022,
 address = {Abu Dhabi, United Arab Emirates},
 author = {Zhang, Yizhe  and
Cai, Deng},
 booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2022.emnlp-main.24},
 editor = {Goldberg, Yoav  and
Kozareva, Zornitsa  and
Zhang, Yue},
 pages = {346--359},
 publisher = {Association for Computational Linguistics},
 title = {Linearizing Transformer with Key-Value Memory},
 url = {https://aclanthology.org/2022.emnlp-main.24},
 year = {2022}
}

@inproceedings{choromanski_rethinking_2021,
 author = {Krzysztof Marcin Choromanski and
Valerii Likhosherstov and
David Dohan and
Xingyou Song and
Andreea Gane and
Tam{\'{a}}s Sarl{\'{o}}s and
Peter Hawkins and
Jared Quincy Davis and
Afroz Mohiuddin and
Lukasz Kaiser and
David Benjamin Belanger and
Lucy J. Colwell and
Adrian Weller},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/ChoromanskiLDSG21.bib},
 booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
Virtual Event, Austria, May 3-7, 2021},
 publisher = {OpenReview.net},
 timestamp = {Wed, 23 Jun 2021 01:00:00 +0200},
 title = {Rethinking Attention with Performers},
 url = {https://openreview.net/forum?id=Ua6zuk0WRH},
 year = {2021}
}

@misc{zhang_hedgehog_2024,
 author = {Zhang, Michael and Bhatia, Kush and Kumbong, Hermann and Ré, Christopher},
 journal = {ArXiv preprint},
 title = {The {Hedgehog} \& the {Porcupine}: {Expressive} {Linear} {Attentions} with {Softmax} {Mimicry}},
 url = {https://arxiv.org/abs/2402.04347},
 volume = {abs/2402.04347},
 year = {2024}
}

@inproceedings{qin_cosformer_2022,
 author = {Zhen Qin and
Weixuan Sun and
Hui Deng and
Dongxu Li and
Yunshen Wei and
Baohong Lv and
Junjie Yan and
Lingpeng Kong and
Yiran Zhong},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/QinSDLWLYKZ22.bib},
 booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
 publisher = {OpenReview.net},
 timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
 title = {cosFormer: Rethinking Softmax In Attention},
 url = {https://openreview.net/forum?id=Bl8CQrx2Up4},
 year = {2022}
}

@misc{nahshan_linear_2023,
 author = {Nahshan, Yury and Kampeas, Joseph and Haleva, Emir},
 journal = {ArXiv preprint},
 title = {Linear {Log}-{Normal} {Attention} with {Unbiased} {Concentration}},
 url = {https://arxiv.org/abs/2311.13541},
 volume = {abs/2311.13541},
 year = {2023}
}

@article{song_expressibility_2023,
 author = {Song, Zhao and Xu, Guangyi and Yin, Junze},
 journal = {ArXiv preprint},
 title = {The {Expressibility} of {Polynomial} based {Attention} {Scheme}},
 url = {https://arxiv.org/abs/2310.20051},
 volume = {abs/2310.20051},
 year = {2023}
}

@article{wang_mambabyte_2024,
 author = {Wang, Junxiong and Gangavarapu, Tushaar and Yan, Jing Nathan and Rush, Alexander M},
 journal = {ArXiv preprint},
 title = {{MambaByte}: {Token}-free {Selective} {State} {Space} {Model}},
 url = {https://arxiv.org/abs/2401.13660},
 volume = {abs/2401.13660},
 year = {2024}
}

@article{xing_segmamba_2024,
 author = {Xing, Zhaohu and Ye, Tian and Yang, Yijun and Liu, Guang and Zhu, Lei},
 journal = {ArXiv preprint},
 title = {Segmamba: {Long}-range sequential modeling mamba for 3d medical image segmentation},
 url = {https://arxiv.org/abs/2401.13560},
 volume = {abs/2401.13560},
 year = {2024}
}

@article{liu_vmamba_2024,
 author = {Liu, Yue and Tian, Yunjie and Zhao, Yuzhong and Yu, Hongtian and Xie, Lingxi and Wang, Yaowei and Ye, Qixiang and Liu, Yunfan},
 journal = {ArXiv preprint},
 title = {Vmamba: {Visual} state space model},
 url = {https://arxiv.org/abs/2401.10166},
 volume = {abs/2401.10166},
 year = {2024}
}

@article{ma_u-mamba_2024,
 author = {Ma, Jun and Li, Feifei and Wang, Bo},
 journal = {ArXiv preprint},
 title = {U-mamba: {Enhancing} long-range dependency for biomedical image segmentation},
 url = {https://arxiv.org/abs/2401.04722},
 volume = {abs/2401.04722},
 year = {2024}
}

@article{yang_vivim_2024,
 author = {Yang, Yijun and Xing, Zhaohu and Zhu, Lei},
 journal = {ArXiv preprint},
 title = {Vivim: a {Video} {Vision} {Mamba} for {Medical} {Video} {Object} {Segmentation}},
 url = {https://arxiv.org/abs/2401.14168},
 volume = {abs/2401.14168},
 year = {2024}
}

@article{oren_transformers_2024,
 author = {Oren, Matanel and Hassid, Michael and Adi, Yossi and Schwartz, Roy},
 journal = {ArXiv preprint},
 title = {Transformers are {Multi}-{State} {RNNs}},
 url = {https://arxiv.org/abs/2401.06104},
 volume = {abs/2401.06104},
 year = {2024}
}

@article{wang_graph-mamba_2024,
 author = {Wang, Chloe and Tsepa, Oleksii and Ma, Jun and Wang, Bo},
 journal = {ArXiv preprint},
 title = {Graph-{Mamba}: {Towards} {Long}-{Range} {Graph} {Sequence} {Modeling} with {Selective} {State} {Spaces}},
 url = {https://arxiv.org/abs/2402.00789},
 volume = {abs/2402.00789},
 year = {2024}
}

@article{zhu_vision_2024,
 author = {Zhu, Lianghui and Liao, Bencheng and Zhang, Qian and Wang, Xinlong and Liu, Wenyu and Wang, Xinggang},
 journal = {ArXiv preprint},
 title = {Vision mamba: {Efficient} visual representation learning with bidirectional state space model},
 url = {https://arxiv.org/abs/2401.09417},
 volume = {abs/2401.09417},
 year = {2024}
}

@article{arora_simple_2024,
 author = {Arora, Simran and Eyuboglu, Sabri and Zhang, Michael and Timalsina, Aman and Alberti, Silas and Zinsley, Dylan and Zou, James and Rudra, Atri and Ré, Christopher},
 journal = {ArXiv preprint},
 title = {Simple linear attention language models balance the recall-throughput tradeoff},
 url = {https://arxiv.org/abs/2402.18668},
 volume = {abs/2402.18668},
 year = {2024}
}

@article{wen_rnns_2024,
 author = {Wen, Kaiyue and Dang, Xingyu and Lyu, Kaifeng},
 journal = {ArXiv preprint},
 title = {{RNNs} are not {Transformers} ({Yet}): {The} {Key} {Bottleneck} on {In}-context {Retrieval}},
 url = {https://arxiv.org/abs/2402.18510},
 volume = {abs/2402.18510},
 year = {2024}
}

@inproceedings{munkhdalai_leave_2024,
 author = {Munkhdalai, Tsendsuren and Faruqui, Manaal and Gopal, Siddharth},
 title = {Leave {No} {Context} {Behind}: {Efficient} {Infinite} {Context} {Transformers} with {Infini}-attention},
 url = {https://api.semanticscholar.org/CorpusID:269033427},
 year = {2024}
}

@article{yang_gated_2023,
 author = {Yang, Songlin and Wang, Bailin and Shen, Yikang and Panda, Rameswar and Kim, Yoon},
 journal = {ArXiv preprint},
 title = {Gated linear attention transformers with hardware-efficient training},
 url = {https://arxiv.org/abs/2312.06635},
 volume = {abs/2312.06635},
 year = {2023}
}

@inproceedings{bischof_wy_1985,
 author = {Bischof, Christian H. and Loan, Charles Van},
 booktitle = {{SIAM} {Conference} on {Parallel} {Processing} for {Scientific} {Computing}},
 title = {The {WY} representation for products of householder matrices},
 url = {https://api.semanticscholar.org/CorpusID:36094006},
 year = {1985}
}

@inproceedings{widrow_adaptive_1988,
 author = {Widrow, Bernard and Hoff, Marcian E and others},
 booktitle = {IRE WESCON convention record},
 number = {1},
 organization = {New York},
 pages = {96--104},
 title = {Adaptive switching circuits},
 volume = {4},
 year = {1960}
}

@article{jelassi_repeat_2024,
 author = {Jelassi, Samy and Brandfonbrener, David and Kakade, Sham M. and Malach, Eran},
 journal = {ArXiv preprint},
 title = {Repeat {After} {Me}: {Transformers} are {Better} than {State} {Space} {Models} at {Copying}},
 url = {https://arxiv.org/abs/2402.01032},
 volume = {abs/2402.01032},
 year = {2024}
}

@inproceedings{qin_hgrn2_2024,
 author = {Qin, Zhen and Yang, Songlin and Sun, Weixuan and Shen, Xuyang and Li, Dong and Sun, Weigao and Zhong, Yiran},
 title = {{HGRN2}: {Gated} {Linear} {RNNs} with {State} {Expansion}},
 url = {https://api.semanticscholar.org/CorpusID:269043328},
 year = {2024}
}
