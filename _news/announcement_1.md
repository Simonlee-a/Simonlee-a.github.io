---
layout: post
date: 2024-05-01 20:00:00-0400
inline: true
related_posts: false
---

Gated Linear Attention Transformers ([GLA](https://arxiv.org/abs/2312.06635)) is accepted to ICML 2024 :smile: Code is available at [here](https://github.com/sustcsonglin/flash-linear-attention/tree/main/fla/models/gla).
